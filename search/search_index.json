{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Why Singularity ? A secure, single-file based container format SIF\u2122 (Singularity Image Format) is a single executable file based container image, cryptographically signed, auditable, secure, and easy to move using existing data mobility paradigms. Support for data-intensive workloads The elegance of Singularity's architecture bridges the gap between HPC and AI, deep learning/machine learning, and predictive analytics. Extreme mobility Use standard file and object copy tools to transport, share, or distribute a Singularity container. Any endpoint with Singularity installed can run the container. Compatibility Designed to support complex architectures and workflows, Singularity is easily adaptable to almost any environment. More ... More from the \"User documentation\" ... What is Singularity. Singularity is not the only OS level virtualization implementation arround. One of the main uses of Singularity is to bring containers and reproducibility to scientific computing and the high-performance computing (HPC) world 1 . More on Wikipedia What is Singularity - an alternative view. Singularity runs in the user space i.e. which allows you to run Singularity containers in systems where you have only user rights - common situation on public and government computer resources. Since your home folder gets automatically mounted/exposed to your virtual environment you can look at it as an alternative way to expose your data to different complete setups with pre-installed and configured software . Purpose This workshop material aims to demonstrate and exercise some commonly used features by simple interactive tutorials. Thus, this is not complete manual or documentation for Singularity. The \" User documentation \" is an excellent reference source with basic examples in well-ordered fashion and always up to date. Kurtzer, Gregory M; Sochat, Vanessa; Bauer, Michael W (2017). \"Singularity: Scientific containers for mobility of compute\". PLOS ONE. 12 (5): e0177459. Bibcode:2017PLoSO..1277459K. doi:10.1371/journal.pone.0177459 . PMC 5426675 . PMID 28494014 \u21a9","title":"Home"},{"location":"#why-singularity","text":"A secure, single-file based container format SIF\u2122 (Singularity Image Format) is a single executable file based container image, cryptographically signed, auditable, secure, and easy to move using existing data mobility paradigms. Support for data-intensive workloads The elegance of Singularity's architecture bridges the gap between HPC and AI, deep learning/machine learning, and predictive analytics. Extreme mobility Use standard file and object copy tools to transport, share, or distribute a Singularity container. Any endpoint with Singularity installed can run the container. Compatibility Designed to support complex architectures and workflows, Singularity is easily adaptable to almost any environment. More ... More from the \"User documentation\" ...","title":"Why Singularity?"},{"location":"#what-is-singularity","text":"Singularity is not the only OS level virtualization implementation arround. One of the main uses of Singularity is to bring containers and reproducibility to scientific computing and the high-performance computing (HPC) world 1 . More on Wikipedia","title":"What is Singularity."},{"location":"#what-is-singularity-an-alternative-view","text":"Singularity runs in the user space i.e. which allows you to run Singularity containers in systems where you have only user rights - common situation on public and government computer resources. Since your home folder gets automatically mounted/exposed to your virtual environment you can look at it as an alternative way to expose your data to different complete setups with pre-installed and configured software .","title":"What is Singularity - an alternative view."},{"location":"#purpose","text":"This workshop material aims to demonstrate and exercise some commonly used features by simple interactive tutorials. Thus, this is not complete manual or documentation for Singularity. The \" User documentation \" is an excellent reference source with basic examples in well-ordered fashion and always up to date. Kurtzer, Gregory M; Sochat, Vanessa; Bauer, Michael W (2017). \"Singularity: Scientific containers for mobility of compute\". PLOS ONE. 12 (5): e0177459. Bibcode:2017PLoSO..1277459K. doi:10.1371/journal.pone.0177459 . PMC 5426675 . PMID 28494014 \u21a9","title":"Purpose"},{"location":"PyTorch_NVIDIA/","text":"NVIDIA Deep Learning Frameworks Here is how easy one can use an NVIDIA environment for deep learning with all the following tools preset. First, Let's pull the container (6.5GB). singularity pull docker://nvcr.io/nvidia/pytorch:22.03-py3 When done, get an interactive shell. singularity shell --nv ~/external_1TB/tmp/pytorch_22.03-py3.sif Singularity> python3 Python 3 .8.12 | packaged by conda-forge | ( default, Jan 30 2022 , 23 :42:07 ) [ GCC 9 .4.0 ] on linux Type \"help\" , \"copyright\" , \"credits\" or \"license\" for more information. >>> import torch # Check torch version >>> print ( torch.__version__ ) 1 .12.0a0+2c916ef # Check if CUDA is available >>> print ( torch.cuda.is_available ()) True # Check which GPU architectures are supported >>> print ( torch.cuda.get_arch_list ()) [ 'sm_52' , 'sm_60' , 'sm_61' , 'sm_70' , 'sm_75' , 'sm_80' , 'sm_86' , 'compute_86' ] # test torch >>> torch.zeros ( 1 ) .to ( 'cuda' ) tensor ([ 0 . ] , device = 'cuda:0' ) From the container shell, check what else is available... Singularity> nvcc -V nvcc: NVIDIA ( R ) Cuda compiler driver Copyright ( c ) 2005 -2022 NVIDIA Corporation Built on Thu_Feb_10_18:23:41_PST_2022 Cuda compilation tools, release 11 .6, V11.6.112 Build cuda_11.6.r11.6/compiler.30978841_0 # Check what conda packages are already there Singularity> conda list -v # Start a jupyter-lab (keep in mind the hostname) Singularity> jupyter-lab ... [ I 13 :35:46.270 LabApp ] [ jupyter_nbextensions_configurator ] enabled 0 .4.1 [ I 13 :35:46.611 LabApp ] jupyter_tensorboard extension loaded. [ I 13 :35:46.615 LabApp ] JupyterLab extension loaded from /opt/conda/lib/python3.8/site-packages/jupyterlab [ I 13 :35:46.615 LabApp ] JupyterLab application directory is /opt/conda/share/jupyter/lab [ I 13 :35:46.616 LabApp ] [ Jupytext Server Extension ] NotebookApp.contents_manager_class is ( a subclass of ) jupytext.TextFileContentsManager already - OK [ I 13 :35:46.616 LabApp ] Serving notebooks from local directory: /home/pmitev [ I 13 :35:46.616 LabApp ] Jupyter Notebook 6 .4.8 is running at: [ I 13 :35:46.616 LabApp ] http://hostname:8888/?token = d6e865a937e527ff5bbccfb3f150480b76566f47eb3808b1 [ I 13 :35:46.616 LabApp ] Use Control-C to stop this server and shut down all kernels ( twice to skip confirmation ) . ... You can use this container to add more packages. Bootstrap : docker From : nvcr.io/nvidia/pytorch: 22.03 -py3 ... Just keep in mind that \"upgrading\" the build-in torch package might install a package that is compatible with less GPU architectures and it might not work anymore on your hardware. Singularity> python3 -c \"import torch; print(torch.__version__); print(torch.cuda.is_available()); print(torch.cuda.get_arch_list()); torch.zeros(1).to('cuda')\" 1 .10.0+cu102 True [ 'sm_37' , 'sm_50' , 'sm_60' , 'sm_70' ] NVIDIA A100-PCIE-40GB with CUDA capability sm_80 is not compatible with the current PyTorch installation. The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70.","title":"NVIDIA Deep Learning Frameworks"},{"location":"PyTorch_NVIDIA/#nvidia-deep-learning-frameworks","text":"Here is how easy one can use an NVIDIA environment for deep learning with all the following tools preset. First, Let's pull the container (6.5GB). singularity pull docker://nvcr.io/nvidia/pytorch:22.03-py3 When done, get an interactive shell. singularity shell --nv ~/external_1TB/tmp/pytorch_22.03-py3.sif Singularity> python3 Python 3 .8.12 | packaged by conda-forge | ( default, Jan 30 2022 , 23 :42:07 ) [ GCC 9 .4.0 ] on linux Type \"help\" , \"copyright\" , \"credits\" or \"license\" for more information. >>> import torch # Check torch version >>> print ( torch.__version__ ) 1 .12.0a0+2c916ef # Check if CUDA is available >>> print ( torch.cuda.is_available ()) True # Check which GPU architectures are supported >>> print ( torch.cuda.get_arch_list ()) [ 'sm_52' , 'sm_60' , 'sm_61' , 'sm_70' , 'sm_75' , 'sm_80' , 'sm_86' , 'compute_86' ] # test torch >>> torch.zeros ( 1 ) .to ( 'cuda' ) tensor ([ 0 . ] , device = 'cuda:0' ) From the container shell, check what else is available... Singularity> nvcc -V nvcc: NVIDIA ( R ) Cuda compiler driver Copyright ( c ) 2005 -2022 NVIDIA Corporation Built on Thu_Feb_10_18:23:41_PST_2022 Cuda compilation tools, release 11 .6, V11.6.112 Build cuda_11.6.r11.6/compiler.30978841_0 # Check what conda packages are already there Singularity> conda list -v # Start a jupyter-lab (keep in mind the hostname) Singularity> jupyter-lab ... [ I 13 :35:46.270 LabApp ] [ jupyter_nbextensions_configurator ] enabled 0 .4.1 [ I 13 :35:46.611 LabApp ] jupyter_tensorboard extension loaded. [ I 13 :35:46.615 LabApp ] JupyterLab extension loaded from /opt/conda/lib/python3.8/site-packages/jupyterlab [ I 13 :35:46.615 LabApp ] JupyterLab application directory is /opt/conda/share/jupyter/lab [ I 13 :35:46.616 LabApp ] [ Jupytext Server Extension ] NotebookApp.contents_manager_class is ( a subclass of ) jupytext.TextFileContentsManager already - OK [ I 13 :35:46.616 LabApp ] Serving notebooks from local directory: /home/pmitev [ I 13 :35:46.616 LabApp ] Jupyter Notebook 6 .4.8 is running at: [ I 13 :35:46.616 LabApp ] http://hostname:8888/?token = d6e865a937e527ff5bbccfb3f150480b76566f47eb3808b1 [ I 13 :35:46.616 LabApp ] Use Control-C to stop this server and shut down all kernels ( twice to skip confirmation ) . ... You can use this container to add more packages. Bootstrap : docker From : nvcr.io/nvidia/pytorch: 22.03 -py3 ... Just keep in mind that \"upgrading\" the build-in torch package might install a package that is compatible with less GPU architectures and it might not work anymore on your hardware. Singularity> python3 -c \"import torch; print(torch.__version__); print(torch.cuda.is_available()); print(torch.cuda.get_arch_list()); torch.zeros(1).to('cuda')\" 1 .10.0+cu102 True [ 'sm_37' , 'sm_50' , 'sm_60' , 'sm_70' ] NVIDIA A100-PCIE-40GB with CUDA capability sm_80 is not compatible with the current PyTorch installation. The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70.","title":"NVIDIA Deep Learning Frameworks"},{"location":"about/","text":"About This web page is a supplementary material to the UPPMAX Singularity workshop. Material related to the workshop Latest course information . Singularity user guildes at SNIC centers UPPMAX NSC HPC2N PDC C3SE Course feedback 2021.05 workshop feedback - including suggestions for improvements . Contacts: Pavlin Mitev - UPPMAX Pedro Ojeda May - HPC2N","title":"About"},{"location":"about/#about","text":"This web page is a supplementary material to the UPPMAX Singularity workshop.","title":"About"},{"location":"about/#material-related-to-the-workshop","text":"Latest course information .","title":"Material related to the workshop"},{"location":"about/#singularity-user-guildes-at-snic-centers","text":"UPPMAX NSC HPC2N PDC C3SE","title":"Singularity user guildes at SNIC centers"},{"location":"about/#course-feedback","text":"2021.05 workshop feedback - including suggestions for improvements .","title":"Course feedback"},{"location":"about/#contacts","text":"Pavlin Mitev - UPPMAX Pedro Ojeda May - HPC2N","title":"Contacts:"},{"location":"build_container/","text":"Building your first container In this simple example we will build Singularity container that will run the following programs fortune | cowsay | lolcat by installing all necessary libraries and packages within Ubuntu 16.04 Linux distribution setup. Simple Singularity definition file Singularity.lolcow BootStrap : docker From : ubuntu: 16.04 %post apt-get -y update apt-get -y install fortune cowsay lolcat %environment export LC_ALL = C export PATH = /usr/games: $PATH %runscript fortune | cowsay | lolcat Building the container $ sudo singularity build lolcow.sif Singularity.lolcow Starting build... Getting image source signatures Copying blob 4007a89234b4 done Copying blob 5dfa26c6b9c9 done Copying blob 0ba7bf18aa40 done Copying blob 4c6ec688ebe3 done Copying config 24336f603e done Writing manifest to image destination Storing signatures ... INFO: Adding environment to container INFO: Adding runscript INFO: Creating SIF file... INFO: Build complete: lolcow.sif Run the Singularity container $ ./lolcow.sif _________________________________________ / You will stop at nothing to reach your \\ | objective, but only because your brakes | \\ are defective. / ----------------------------------------- \\ ^__^ \\ (oo)\\_______ (__)\\ )\\/\\ ||----w | || || -=>>> Done <<<=- asciinema Syntax of the definition file Singularity.lolcow 1 2 3 4 5 6 7 8 9 10 11 12 13 BootStrap : docker From : ubuntu: 16.04 %post apt-get -y update apt-get -y install fortune cowsay lolcat %environment export LC_ALL = C export PATH = /usr/games: $PATH %runscript fortune | cowsay | lolcat header : Bootsrap agents - online documentation library - images hosted on the Container Library docker - images hosted on Docker Hub shub - images hosted on Singularity Hub ... Other: localimage , yum , debootstrap , oci , oci-archive , docker-daemon , docker-archive , arch , busybox , zypper header : From Depending on the value assigned to Bootstrap , other keywords may also be valid in the header. For example, when using the library bootstrap agent, the From keyword becomes valid. %post This section is where you can download files from the Internet, install new software and libraries, write configuration files, create new directories, etc. %environment The %environment section allows you to define environment variables that will be set at runtime. %runscript The contents of the %runscript section are written to a file within the container that is executed when the container image is run (either via the singularity run command or by executing the container directly as a command) Brief summary with examples - online documentation All sections - online documentation Other simple examples TeX Live You need to use recent TeX Live but you do not want to (or you can't) upgrade your OS to the latest release... or you do not want to struggle unnecessary to make manual installation that has countless dependencies... Singularity.texlive Bootstrap : docker From : ubuntu:latest %post export DEBIAN_FRONTEND = noninteractive apt-get update && apt-get -y dist-upgrade && apt-get install -y texlive-full && apt-get clean %runscript /bin/bash \" $@ \" $ sudo singularity build texlive.sif Singularity.texlive $ ./texlive.sif In this case, running the container will bring you to the bash prompt within latest Ubuntu release and the corresponding texlive-full package. Keep in mind that the shell is running within the container and this setup does not have any other tools like git , wget , vim etc. If you want them available in the container, do you know where to add them? Installing software from a local package Sometimes, you cannot download a package directly or the software needs signing licenses. In this case you need to push in the locally downloaded file during the build process. You can get the latest version of the file bellow from here: https://jp-minerals.org/vesta/en/download.htm ( download the the linux VESTA-gtk3.tar.bz2 version ). Singularity.vesta Bootstrap : docker From : ubuntu: 20.04 %files VESTA-gtk3.tar.bz2 / %post export DEBIAN_FRONTEND = noninteractive apt-get update && apt-get -y dist-upgrade && \\ apt-get install -y libxmu6 libxss1 libxft2 libquadmath0 libpng16-16 bzip2 libgl1-mesa-glx \\ libglu1-mesa libglib2.0-0 libgtk-3-0 libgtk-3-dev libgomp1 && \\ apt-get clean cd / tar -C /usr/local -xvf /VESTA-gtk3.tar.bz2 && rm /VESTA-gtk3.tar.bz2 %runscript /usr/local/VESTA-gtk3/VESTA \" $@ \" Note the %files section. The line bellow will copy VESTA-gtk3.tar.bz2 from the current directory to the root folder \\ in the Singularity container. Also, you need to figure out yourself all required libraries and dependencies and install them.","title":"Building a container"},{"location":"build_container/#building-your-first-container","text":"In this simple example we will build Singularity container that will run the following programs fortune | cowsay | lolcat by installing all necessary libraries and packages within Ubuntu 16.04 Linux distribution setup.","title":"Building your first container"},{"location":"build_container/#simple-singularity-definition-file","text":"Singularity.lolcow BootStrap : docker From : ubuntu: 16.04 %post apt-get -y update apt-get -y install fortune cowsay lolcat %environment export LC_ALL = C export PATH = /usr/games: $PATH %runscript fortune | cowsay | lolcat","title":"Simple Singularity definition file"},{"location":"build_container/#building-the-container","text":"$ sudo singularity build lolcow.sif Singularity.lolcow Starting build... Getting image source signatures Copying blob 4007a89234b4 done Copying blob 5dfa26c6b9c9 done Copying blob 0ba7bf18aa40 done Copying blob 4c6ec688ebe3 done Copying config 24336f603e done Writing manifest to image destination Storing signatures ... INFO: Adding environment to container INFO: Adding runscript INFO: Creating SIF file... INFO: Build complete: lolcow.sif","title":"Building the container"},{"location":"build_container/#run-the-singularity-container","text":"$ ./lolcow.sif _________________________________________ / You will stop at nothing to reach your \\ | objective, but only because your brakes | \\ are defective. / ----------------------------------------- \\ ^__^ \\ (oo)\\_______ (__)\\ )\\/\\ ||----w | || || -=>>> Done <<<=- asciinema","title":"Run the Singularity container"},{"location":"build_container/#syntax-of-the-definition-file","text":"Singularity.lolcow 1 2 3 4 5 6 7 8 9 10 11 12 13 BootStrap : docker From : ubuntu: 16.04 %post apt-get -y update apt-get -y install fortune cowsay lolcat %environment export LC_ALL = C export PATH = /usr/games: $PATH %runscript fortune | cowsay | lolcat","title":"Syntax of the definition file"},{"location":"build_container/#header-bootsrap-agents-online-documentation","text":"library - images hosted on the Container Library docker - images hosted on Docker Hub shub - images hosted on Singularity Hub ... Other: localimage , yum , debootstrap , oci , oci-archive , docker-daemon , docker-archive , arch , busybox , zypper","title":"header: Bootsrap agents - online documentation"},{"location":"build_container/#header-from","text":"Depending on the value assigned to Bootstrap , other keywords may also be valid in the header. For example, when using the library bootstrap agent, the From keyword becomes valid.","title":"header: From"},{"location":"build_container/#post","text":"This section is where you can download files from the Internet, install new software and libraries, write configuration files, create new directories, etc.","title":"%post"},{"location":"build_container/#environment","text":"The %environment section allows you to define environment variables that will be set at runtime.","title":"%environment"},{"location":"build_container/#runscript","text":"The contents of the %runscript section are written to a file within the container that is executed when the container image is run (either via the singularity run command or by executing the container directly as a command)","title":"%runscript"},{"location":"build_container/#brief-summary-with-examples-online-documentation","text":"","title":"Brief summary with examples - online documentation"},{"location":"build_container/#all-sections-online-documentation","text":"","title":"All sections - online documentation"},{"location":"build_container/#other-simple-examples","text":"","title":"Other simple examples"},{"location":"build_container/#tex-live","text":"You need to use recent TeX Live but you do not want to (or you can't) upgrade your OS to the latest release... or you do not want to struggle unnecessary to make manual installation that has countless dependencies... Singularity.texlive Bootstrap : docker From : ubuntu:latest %post export DEBIAN_FRONTEND = noninteractive apt-get update && apt-get -y dist-upgrade && apt-get install -y texlive-full && apt-get clean %runscript /bin/bash \" $@ \" $ sudo singularity build texlive.sif Singularity.texlive $ ./texlive.sif In this case, running the container will bring you to the bash prompt within latest Ubuntu release and the corresponding texlive-full package. Keep in mind that the shell is running within the container and this setup does not have any other tools like git , wget , vim etc. If you want them available in the container, do you know where to add them?","title":"TeX Live"},{"location":"build_container/#installing-software-from-a-local-package","text":"Sometimes, you cannot download a package directly or the software needs signing licenses. In this case you need to push in the locally downloaded file during the build process. You can get the latest version of the file bellow from here: https://jp-minerals.org/vesta/en/download.htm ( download the the linux VESTA-gtk3.tar.bz2 version ). Singularity.vesta Bootstrap : docker From : ubuntu: 20.04 %files VESTA-gtk3.tar.bz2 / %post export DEBIAN_FRONTEND = noninteractive apt-get update && apt-get -y dist-upgrade && \\ apt-get install -y libxmu6 libxss1 libxft2 libquadmath0 libpng16-16 bzip2 libgl1-mesa-glx \\ libglu1-mesa libglib2.0-0 libgtk-3-0 libgtk-3-dev libgomp1 && \\ apt-get clean cd / tar -C /usr/local -xvf /VESTA-gtk3.tar.bz2 && rm /VESTA-gtk3.tar.bz2 %runscript /usr/local/VESTA-gtk3/VESTA \" $@ \" Note the %files section. The line bellow will copy VESTA-gtk3.tar.bz2 from the current directory to the root folder \\ in the Singularity container. Also, you need to figure out yourself all required libraries and dependencies and install them.","title":"Installing software from a local package"},{"location":"collections/","text":"List with links to Singularity recipes online https://github.com/sylabs/examples - examples from Sylabs itself https://github.com/accre/singularity - examples that use singularity containers to run arbitrary operating systems on the ACCRE cluster. https://github.com/hpcng/singularity/tree/master/examples - variety of examples","title":"List with links to Singularity recipes online"},{"location":"collections/#list-with-links-to-singularity-recipes-online","text":"https://github.com/sylabs/examples - examples from Sylabs itself https://github.com/accre/singularity - examples that use singularity containers to run arbitrary operating systems on the ACCRE cluster. https://github.com/hpcng/singularity/tree/master/examples - variety of examples","title":"List with links to Singularity recipes online"},{"location":"exercise_00/","text":"Simple build to exercise Let's start with something easy and fast to build - Install the figlet app in a container. $ figlet UPPMAX _ _ ____ ____ __ __ _ __ __ | | | | _ \\| _ \\| \\/ | / \\ \\ \\/ / | | | | |_) | |_) | |\\/| | / _ \\ \\ / | |_| | __/| __/| | | |/ ___ \\ / \\ \\___/|_| |_| |_| |_/_/ \\_\\/_/\\_\\ $ figlet -f slant UPPMAX __ ______ ____ __ ______ _ __ / / / / __ \\/ __ \\/ |/ / | | |/ / / / / / /_/ / /_/ / /|_/ / /| | | / / /_/ / ____/ ____/ / / / ___ |/ | \\____/_/ /_/ /_/ /_/_/ |_/_/|_| Let's use Ubuntu from Docker to install the package. Singularity.figlet Bootstrap: docker From: ubuntu:20.04 It is not necessary right now, but it is good to inform the package manager that we are not in a interactive session by export DEBIAN_FRONTEND=noninteractive We need only to install the figlet package by apt-get. Do not forget to apt-get update first, since the package index is empty. apt-get install -y figlet - use -y to avoid the confirmation when installing packages In what section we add these commands? Let's clean a bit with apt-get clean Singularity.figlet Bootstrap: docker From: ubuntu:20.04 %post export DEBIAN_FRONTEND=noninteractive apt-get update apt-get install -y figlet apt-get clean Let's define what to run when we run the container itself. Singularity.figlet Bootstrap: docker From: ubuntu:20.04 %post export DEBIAN_FRONTEND=noninteractive apt-get update apt-get install -y figlet apt-get clean %runscript figlet \"$@\" Build the recipe build $ sudo singularity build figlet.sif Singularity.figlet","title":"Something easy"},{"location":"exercise_00/#simple-build-to-exercise","text":"Let's start with something easy and fast to build - Install the figlet app in a container. $ figlet UPPMAX _ _ ____ ____ __ __ _ __ __ | | | | _ \\| _ \\| \\/ | / \\ \\ \\/ / | | | | |_) | |_) | |\\/| | / _ \\ \\ / | |_| | __/| __/| | | |/ ___ \\ / \\ \\___/|_| |_| |_| |_/_/ \\_\\/_/\\_\\ $ figlet -f slant UPPMAX __ ______ ____ __ ______ _ __ / / / / __ \\/ __ \\/ |/ / | | |/ / / / / / /_/ / /_/ / /|_/ / /| | | / / /_/ / ____/ ____/ / / / ___ |/ | \\____/_/ /_/ /_/ /_/_/ |_/_/|_| Let's use Ubuntu from Docker to install the package. Singularity.figlet Bootstrap: docker From: ubuntu:20.04 It is not necessary right now, but it is good to inform the package manager that we are not in a interactive session by export DEBIAN_FRONTEND=noninteractive We need only to install the figlet package by apt-get. Do not forget to apt-get update first, since the package index is empty. apt-get install -y figlet - use -y to avoid the confirmation when installing packages In what section we add these commands? Let's clean a bit with apt-get clean Singularity.figlet Bootstrap: docker From: ubuntu:20.04 %post export DEBIAN_FRONTEND=noninteractive apt-get update apt-get install -y figlet apt-get clean Let's define what to run when we run the container itself. Singularity.figlet Bootstrap: docker From: ubuntu:20.04 %post export DEBIAN_FRONTEND=noninteractive apt-get update apt-get install -y figlet apt-get clean %runscript figlet \"$@\" Build the recipe build $ sudo singularity build figlet.sif Singularity.figlet","title":"Simple build to exercise"},{"location":"exercise_01/","text":"Try to compile Singularity recipe Here is a real-life example - you want to run gapseq tool with Singularity. https://gapseq.readthedocs.io/en/latest/install.html gapseq is a program for the prediction and analysis of metabolic pathways and genome-scale networks. Create new folder for this project. Use the Ubuntu installation instructions. 1 2 3 4 sudo apt install ncbi-blast+ git libglpk-dev r-base-core exonerate bedtools barrnap bc R -e 'install.packages(c(\"data.table\", \"stringr\", \"sybil\", \"getopt\", \"reshape2\", \"doParallel\", \"foreach\", \"R.utils\", \"stringi\", \"glpkAPI\", \"CHNOSZ\", \"jsonlite\"))' R -e 'if (!requireNamespace(\"BiocManager\", quietly = TRUE)) install.packages(\"BiocManager\"); BiocManager::install(\"Biostrings\")' git clone https://github.com/jotech/gapseq && cd gapseq Do not install the SBML tool ( not in the above instructions anyway ). Think (discuss) where to clone the GitHUB repository from line 4. Note that this particular tool downloads external data into the repository structure, which does not work if you include add the repository in the container itself (the common container format is read-only). Thus, the cloning of the repository should be done in your home or project folder where you can run the program with the long syntax i.e. $ singularity exec ../gapseq.sif ./gapseq doall toy/ecoli.fna.gz Start the build and save the output to a file to track down potential errors $ sudo singularity build ... |& tee build.log Clone the git repository - line 4 $ git clone https://github.com/jotech/gapseq && cd gapseq Test the container by running the tool that will start the gapseq tool from the github repository. $ singularity exec ../gapseq.sif ./gapseq output $ singularity exec ../gapseq.sif ./gapseq __ _ __ _ _ __ ___ ___ __ _ / _` |/ _` | '_ \\/ __|/ _ \\/ _` | | (_| | (_| | |_) \\__ \\ __/ (_| | \\__, |\\__,_| .__/|___/\\___|\\__, | |___/ |_| |_| Informed prediction and analysis of bacterial metabolic pathways and genome-scale networks Usage: gapseq test gapseq (find | find-transport | draft | fill | doall | adapt) (.. .) gapseq doall (genome) [medium] [Bacteria|Archaea] gapseq find (-p pathways | -e enzymes) [-b bitscore] (genome) gapseq find-transport [-b bitscore] (genome) gapseq draft (-r reactions | -t transporter -c genome -p pathways) [-b pos|neg|archaea|auto] gapseq fill (-m draft -n medium -c rxn_weights -g rxn_genes) gapseq adapt (add | remove) (reactions,pathways) (model) Examples: gapseq test gapseq doall toy/ecoli.fna.gz gapseq doall toy/myb71.fna.gz dat/media/TSBmed.csv gapseq find -p chitin toy/myb71.fna.gz gapseq find -p all toy/myb71.fna.gz gapseq find-transport toy/myb71.fna.gz gapseq draft -r toy/ecoli-all-Reactions.tbl -t toy/ ecoli-Transporter.tbl -c toy/ecoli.fna.gz -p toy/ ecoli-all-Pathways.tbl gapseq fill -m toy/ecoli-draft.RDS -n dat/media/ALLmed.csv -c toy/ecoli-rxnWeights.RDS -g toy/ecoli-rxnXgenes.RDS gapseq adapt add 14DICHLORBENZDEG-PWY toy/myb71.RDS Options: test Testing dependencies and basic functionality of gapseq. find Pathway analysis, try to find enzymes based on homology. find-transport Search for transporters based on homology. draft Draft model construction based on results from find and find-transport. fill Gap filling of a model. doall Combine find, find-transport, draft and fill. adapt Add or remove reactions or pathways. -v Show version. -h Show this screen. -n Enable noisy verbose mode. Try to run singularity exec ../gapseq.sif ./gapseq test . Did it pass the tests? What is wrong? The output below shows an output with solved R packages tests. The second problem is related to the repository itself. output $ singularity exec ../gapseq.sif ./gapseq test gapseq version: 1.1 7c25ca2 linux-gnu #74-Ubuntu SMP Wed Jan 27 22:54:38 UTC 2021 ####################### #Checking dependencies# ####################### GNU Awk 5.0.1, API: 2.0 (GNU MPFR 4.0.2, GNU MP 6.2.0) sed (GNU sed) 4.7 grep (GNU grep) 3.4 This is perl 5, version 30, subversion 0 (v5.30.0) built for x86_64-linux-gnu-thread-multi tblastn: 2.9.0+ exonerate from exonerate version 2.4.0 bedtools v2.27.1 barrnap 0.9 - rapid ribosomal RNA prediction R version 3.6.3 (2020-02-29) -- \"Holding the Windsock\" R scripting front-end version 3.6.3 (2020-02-29) git version 2.25.1 Missing dependencies: 0 ##################### #Checking R packages# ##################### data.table 1.14.0 stringr 1.4.0 sybil 2.1.5 getopt 1.20.3 reshape2 1.4.4 doParallel 1.0.16 foreach 1.5.1 R.utils 2.10.1 stringi 1.5.3 glpkAPI 1.3.2 BiocManager 1.30.10 Biostrings 2.54.0 jsonlite 1.7.2 CHNOSZ 1.4.0 Missing R packages: 0 ############################## #Checking basic functionality# ############################## Optimization test: OK Command line argument error: Argument \"query\". File is not accessible: `/opt/gapseq/src/../dat/seq/Bacteria/rev/1.2.4.1. fasta' Blast test: FAILED Passed tests: 1/2 Here is a working recipe for the exercise: https://github.com/pmitev/UPPMAX-Singularity/tree/main/gapseq","title":"Real-life example"},{"location":"exercise_01/#try-to-compile-singularity-recipe","text":"Here is a real-life example - you want to run gapseq tool with Singularity. https://gapseq.readthedocs.io/en/latest/install.html gapseq is a program for the prediction and analysis of metabolic pathways and genome-scale networks. Create new folder for this project. Use the Ubuntu installation instructions. 1 2 3 4 sudo apt install ncbi-blast+ git libglpk-dev r-base-core exonerate bedtools barrnap bc R -e 'install.packages(c(\"data.table\", \"stringr\", \"sybil\", \"getopt\", \"reshape2\", \"doParallel\", \"foreach\", \"R.utils\", \"stringi\", \"glpkAPI\", \"CHNOSZ\", \"jsonlite\"))' R -e 'if (!requireNamespace(\"BiocManager\", quietly = TRUE)) install.packages(\"BiocManager\"); BiocManager::install(\"Biostrings\")' git clone https://github.com/jotech/gapseq && cd gapseq Do not install the SBML tool ( not in the above instructions anyway ). Think (discuss) where to clone the GitHUB repository from line 4. Note that this particular tool downloads external data into the repository structure, which does not work if you include add the repository in the container itself (the common container format is read-only). Thus, the cloning of the repository should be done in your home or project folder where you can run the program with the long syntax i.e. $ singularity exec ../gapseq.sif ./gapseq doall toy/ecoli.fna.gz Start the build and save the output to a file to track down potential errors $ sudo singularity build ... |& tee build.log Clone the git repository - line 4 $ git clone https://github.com/jotech/gapseq && cd gapseq Test the container by running the tool that will start the gapseq tool from the github repository. $ singularity exec ../gapseq.sif ./gapseq output $ singularity exec ../gapseq.sif ./gapseq __ _ __ _ _ __ ___ ___ __ _ / _` |/ _` | '_ \\/ __|/ _ \\/ _` | | (_| | (_| | |_) \\__ \\ __/ (_| | \\__, |\\__,_| .__/|___/\\___|\\__, | |___/ |_| |_| Informed prediction and analysis of bacterial metabolic pathways and genome-scale networks Usage: gapseq test gapseq (find | find-transport | draft | fill | doall | adapt) (.. .) gapseq doall (genome) [medium] [Bacteria|Archaea] gapseq find (-p pathways | -e enzymes) [-b bitscore] (genome) gapseq find-transport [-b bitscore] (genome) gapseq draft (-r reactions | -t transporter -c genome -p pathways) [-b pos|neg|archaea|auto] gapseq fill (-m draft -n medium -c rxn_weights -g rxn_genes) gapseq adapt (add | remove) (reactions,pathways) (model) Examples: gapseq test gapseq doall toy/ecoli.fna.gz gapseq doall toy/myb71.fna.gz dat/media/TSBmed.csv gapseq find -p chitin toy/myb71.fna.gz gapseq find -p all toy/myb71.fna.gz gapseq find-transport toy/myb71.fna.gz gapseq draft -r toy/ecoli-all-Reactions.tbl -t toy/ ecoli-Transporter.tbl -c toy/ecoli.fna.gz -p toy/ ecoli-all-Pathways.tbl gapseq fill -m toy/ecoli-draft.RDS -n dat/media/ALLmed.csv -c toy/ecoli-rxnWeights.RDS -g toy/ecoli-rxnXgenes.RDS gapseq adapt add 14DICHLORBENZDEG-PWY toy/myb71.RDS Options: test Testing dependencies and basic functionality of gapseq. find Pathway analysis, try to find enzymes based on homology. find-transport Search for transporters based on homology. draft Draft model construction based on results from find and find-transport. fill Gap filling of a model. doall Combine find, find-transport, draft and fill. adapt Add or remove reactions or pathways. -v Show version. -h Show this screen. -n Enable noisy verbose mode. Try to run singularity exec ../gapseq.sif ./gapseq test . Did it pass the tests? What is wrong? The output below shows an output with solved R packages tests. The second problem is related to the repository itself. output $ singularity exec ../gapseq.sif ./gapseq test gapseq version: 1.1 7c25ca2 linux-gnu #74-Ubuntu SMP Wed Jan 27 22:54:38 UTC 2021 ####################### #Checking dependencies# ####################### GNU Awk 5.0.1, API: 2.0 (GNU MPFR 4.0.2, GNU MP 6.2.0) sed (GNU sed) 4.7 grep (GNU grep) 3.4 This is perl 5, version 30, subversion 0 (v5.30.0) built for x86_64-linux-gnu-thread-multi tblastn: 2.9.0+ exonerate from exonerate version 2.4.0 bedtools v2.27.1 barrnap 0.9 - rapid ribosomal RNA prediction R version 3.6.3 (2020-02-29) -- \"Holding the Windsock\" R scripting front-end version 3.6.3 (2020-02-29) git version 2.25.1 Missing dependencies: 0 ##################### #Checking R packages# ##################### data.table 1.14.0 stringr 1.4.0 sybil 2.1.5 getopt 1.20.3 reshape2 1.4.4 doParallel 1.0.16 foreach 1.5.1 R.utils 2.10.1 stringi 1.5.3 glpkAPI 1.3.2 BiocManager 1.30.10 Biostrings 2.54.0 jsonlite 1.7.2 CHNOSZ 1.4.0 Missing R packages: 0 ############################## #Checking basic functionality# ############################## Optimization test: OK Command line argument error: Argument \"query\". File is not accessible: `/opt/gapseq/src/../dat/seq/Bacteria/rev/1.2.4.1. fasta' Blast test: FAILED Passed tests: 1/2 Here is a working recipe for the exercise: https://github.com/pmitev/UPPMAX-Singularity/tree/main/gapseq","title":"Try to compile Singularity recipe"},{"location":"exercise_02/","text":"Build, sign, and upload Singularity container to Sylabs container library. Online material 1. Make an account Go to: https://cloud.sylabs.io/library. Click \"Sign in to Sylabs\" (top right corner). Select your method to sign in, with Google, GitHub, GitLab, or Microsoft. Type your passwords, and that's it! 2. Create an access token and login: link 3. Pushing container $ singularity push my-container.sif library://your-name/project-dir/my-container:latest Info Use lower case for project-dir to avoid strange problems. You might need to use -U or --allow-unsigned to push the container. 4. Sign your container (optional): link","title":"Upload to Sylabs container library"},{"location":"exercise_02/#build-sign-and-upload-singularity-container-to-sylabs-container-library","text":"Online material","title":"Build, sign, and upload Singularity container to Sylabs container library."},{"location":"exercise_02/#1-make-an-account","text":"Go to: https://cloud.sylabs.io/library. Click \"Sign in to Sylabs\" (top right corner). Select your method to sign in, with Google, GitHub, GitLab, or Microsoft. Type your passwords, and that's it!","title":"1. Make an account"},{"location":"exercise_02/#2-create-an-access-token-and-login-link","text":"","title":"2. Create an access token and login: link"},{"location":"exercise_02/#3-pushing-container","text":"$ singularity push my-container.sif library://your-name/project-dir/my-container:latest Info Use lower case for project-dir to avoid strange problems. You might need to use -U or --allow-unsigned to push the container.","title":"3. Pushing container"},{"location":"exercise_02/#4-sign-your-container-optional-link","text":"","title":"4. Sign your container (optional): link"},{"location":"exercise_04/","text":"Run Visual Studio Code with Singularity https://code.visualstudio.com/ This is another real-life scenario. VSCode got rather popular these days, but it is still not available to run on Rackham... Let's try to assemble a recipe and see what difficulties could bring this. For Debian distributions the installation is done via downloading a package .deb file. https://code.visualstudio.com/sha/download?build=stable&os=linux-deb-x64 1. The common... If we choose the \"static\" builds path i.e. not using --sandbox , then we can come up with something like this right away. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 BootStrap : docker From : ubuntu: 20.04 %post export DEBIAN_FRONTEND = noninteractive apt-get -y update apt-get -y install wget git curl cd / wget \"https://code.visualstudio.com/sha/download?build=stable&os=linux-deb-x64\" -O code_stable_amd64.deb apt-get -y install ./code_stable_amd64.deb apt-get clean %environment export LC_ALL = C %runscript /bin/bash lines 1-6 : something trivial. We use docker and Ubuntu.20.04, set export DEBIAN_FRONTEND=noninteractive and update the apt repositories. line 7 : make sure we have some tools since VSCode might need them and they are not that big anyway. lines 9-11 : downloads the latest stable release and we save it as code_stable_amd64.deb . What happens if you do not specify the name of the output file. Try it in the terminal. line 13 : just the usual cleaning. lines 15-16 : Some safe defaults lines 18-19 : At this point we do not know where the executable will be, so we will start just a bash shell instead. Build the recipe, and run it. This will start a shell in the container. Use which code to find the location of the VSCode program. $ sudo singularity build vscode.sif Singularity.vscode $ ./vscode.sif Singularity $ which code While in the container, try to start the editor with code . Most probably you will get an error about a missing dynamic library libX11-xcb.so.1 ( version 1.55.0-1617120720 ) . Seems that this was not in the dependency of the package... Finding the missing pieces This is an easy case - it could be more problematic. Visit https://packages.ubuntu.com/ and use the \"Search the contents of packages\" for focal (Ubuntu 20.04) to find which package could possibly provide the missing library. You should be able to find that libx11-xcb1 package contains this file... So, we need to add it to the apt-get install ... line and rebuild. In the general case there might be more missing libraries and using tools like ldd might come more handy to track down multiple missing libraries. Add the corrections and rebuild add libx11-xcb1 to line 7 replace line 19 with the full path to the program to start /usr/bin/code $@ code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 BootStrap : docker From : ubuntu: 20.04 %post export DEBIAN_FRONTEND = noninteractive apt-get -y update apt-get -y install wget git curl vim libx11-xcb1 libxshmfence1 cd / wget \"https://code.visualstudio.com/sha/download?build=stable&os=linux-deb-x64\" -O code_stable_amd64.deb apt-get -y install ./code_stable_amd64.deb apt-get clean rm code_stable_amd64.deb %environment export LC_ALL = C %runscript /usr/bin/code $@ Run again Unfortunatelly we are not ready. There will be this pop up window with warnings... There might be better ways to do this but at this point we will just give write access to /var to our container. $ singularity run -B /run ./vscode.sif Note that we did not specify the destination of the folder in the container, but the syntax allows it and this will be equivalent to -B /run:/run","title":"VSCode in Singularity"},{"location":"exercise_04/#run-visual-studio-code-with-singularity","text":"https://code.visualstudio.com/ This is another real-life scenario. VSCode got rather popular these days, but it is still not available to run on Rackham... Let's try to assemble a recipe and see what difficulties could bring this. For Debian distributions the installation is done via downloading a package .deb file. https://code.visualstudio.com/sha/download?build=stable&os=linux-deb-x64","title":"Run Visual Studio Code with Singularity"},{"location":"exercise_04/#1-the-common","text":"If we choose the \"static\" builds path i.e. not using --sandbox , then we can come up with something like this right away. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 BootStrap : docker From : ubuntu: 20.04 %post export DEBIAN_FRONTEND = noninteractive apt-get -y update apt-get -y install wget git curl cd / wget \"https://code.visualstudio.com/sha/download?build=stable&os=linux-deb-x64\" -O code_stable_amd64.deb apt-get -y install ./code_stable_amd64.deb apt-get clean %environment export LC_ALL = C %runscript /bin/bash lines 1-6 : something trivial. We use docker and Ubuntu.20.04, set export DEBIAN_FRONTEND=noninteractive and update the apt repositories. line 7 : make sure we have some tools since VSCode might need them and they are not that big anyway. lines 9-11 : downloads the latest stable release and we save it as code_stable_amd64.deb . What happens if you do not specify the name of the output file. Try it in the terminal. line 13 : just the usual cleaning. lines 15-16 : Some safe defaults lines 18-19 : At this point we do not know where the executable will be, so we will start just a bash shell instead. Build the recipe, and run it. This will start a shell in the container. Use which code to find the location of the VSCode program. $ sudo singularity build vscode.sif Singularity.vscode $ ./vscode.sif Singularity $ which code While in the container, try to start the editor with code . Most probably you will get an error about a missing dynamic library libX11-xcb.so.1 ( version 1.55.0-1617120720 ) . Seems that this was not in the dependency of the package...","title":"1. The common..."},{"location":"exercise_04/#finding-the-missing-pieces","text":"This is an easy case - it could be more problematic. Visit https://packages.ubuntu.com/ and use the \"Search the contents of packages\" for focal (Ubuntu 20.04) to find which package could possibly provide the missing library. You should be able to find that libx11-xcb1 package contains this file... So, we need to add it to the apt-get install ... line and rebuild. In the general case there might be more missing libraries and using tools like ldd might come more handy to track down multiple missing libraries.","title":"Finding the missing pieces"},{"location":"exercise_04/#add-the-corrections-and-rebuild","text":"add libx11-xcb1 to line 7 replace line 19 with the full path to the program to start /usr/bin/code $@ code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 BootStrap : docker From : ubuntu: 20.04 %post export DEBIAN_FRONTEND = noninteractive apt-get -y update apt-get -y install wget git curl vim libx11-xcb1 libxshmfence1 cd / wget \"https://code.visualstudio.com/sha/download?build=stable&os=linux-deb-x64\" -O code_stable_amd64.deb apt-get -y install ./code_stable_amd64.deb apt-get clean rm code_stable_amd64.deb %environment export LC_ALL = C %runscript /usr/bin/code $@","title":"Add the corrections and rebuild"},{"location":"exercise_04/#run-again","text":"Unfortunatelly we are not ready. There will be this pop up window with warnings... There might be better ways to do this but at this point we will just give write access to /var to our container. $ singularity run -B /run ./vscode.sif Note that we did not specify the destination of the folder in the container, but the syntax allows it and this will be equivalent to -B /run:/run","title":"Run again"},{"location":"fakeroot/","text":"Building without elevated privileges with --fakeroot Online manual The fakeroot feature (commonly referred as rootless mode) allows an unprivileged user to run a container as a \" fake root \" user by leveraging user namespace UID/GID mapping . A \" fake root \" user has almost the same administrative rights as root but only inside the container and the requested namespaces , which means that this user: can set different user/group ownership for files or directories they own can change user/group identity with su/sudo commands has full privileges inside the requested namespaces (network, ipc, uts) Note Many computer centers, including UPPMAX, does not allow the use of \"fake root\" and attempt to build on Rackham will trigger the following error: $ singularity build --fakeroot lolcow.sif Singularity.lolcow FATAL: could not use fakeroot: no mapping entry found in /etc/subuid for user","title":"Building with --fakeroot"},{"location":"fakeroot/#building-without-elevated-privileges-with-fakeroot","text":"Online manual The fakeroot feature (commonly referred as rootless mode) allows an unprivileged user to run a container as a \" fake root \" user by leveraging user namespace UID/GID mapping . A \" fake root \" user has almost the same administrative rights as root but only inside the container and the requested namespaces , which means that this user: can set different user/group ownership for files or directories they own can change user/group identity with su/sudo commands has full privileges inside the requested namespaces (network, ipc, uts) Note Many computer centers, including UPPMAX, does not allow the use of \"fake root\" and attempt to build on Rackham will trigger the following error: $ singularity build --fakeroot lolcow.sif Singularity.lolcow FATAL: could not use fakeroot: no mapping entry found in /etc/subuid for user","title":"Building without elevated privileges with --fakeroot"},{"location":"goods_bads/","text":"Common good or bad practices for building Singularity containers Disclaimer: these might not be the best solutions at all. Where to compile and install source codes. There are some, not so easy to see, complications. /tmp looks like good choice... The problem is that /tmp is mounted automatically even during the build process. This means that you will collide with leftovers from previous builds which might lead to rather unexpected results. We will use this problematic behavior in the next section for our advantage. $HOME points to /root during build, and it is also mounted at build time. Really bad place to compile! So... Where is a good place to compile and install? Here is an example scenario 1 2 3 4 5 6 7 8 9 10 11 12 13 %environment export PATH = /opt/tool-name/bin: ${ PATH } %post mkdir -p /installs && cd /installs git clone repository-link && cd tool-name ./configure --prefix = /opt/tool-name make && make install ... # Clean the installation folder (unless you want to keep it) rm -rf /installs dedicate a folder in the container's file structure fetch your installation files there (look how this might be improved for large files downloaded with wget ) install in /opt and adjust the $PATH or just allow the tool to mix with the system files. Conda Conda causes some unexpected problems. During the build and and the commands in %runscrupt sections are run with /bin/sh which fails upon source /full_path_to/conda.sh which in turn fails conda activate my_environment . Her are two examples how to deal with the situation. docker://continuumio/miniconda3 container 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 Bootstrap : docker From : continuumio/miniconda3 %environment export LC_ALL = C %post export LC_ALL = C conda config --add channels defaults conda config --add channels conda-forge conda config --add channels bioconda conda config --add channels ursky conda create --name metawrap-env --channel ursky metawrap-mg = 1 .3.2 tbb = 2020 .2 conda clean --all --yes %runscript params = $@ /bin/bash <<EOF source /opt/conda/etc/profile.d/conda.sh conda activate metawrap-env metawrap $params EOF Ubuntu + conda 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 Bootstrap : docker From : ubuntu: 20.04 %labels Author pmitev@gmail.com %environment export LC_ALL = C export CONDA_ENVS_PATH = /opt/conda_envs export PATH = /opt/metaWRAP/bin: $PATH %post export DEBIAN_FRONTEND = noninteractive export LC_ALL = C export CONDA_ENVS_PATH = /opt/conda_envs && mkdir -p ${ CONDA_ENVS_PATH } apt-get update && apt-get -y install wget git mkdir /installs && cd /installs # Conda installation ============================================== mconda = \"Miniconda3-py38_4.9.2-Linux-x86_64.sh\" wget https://repo.anaconda.com/miniconda/ ${ mconda } && \\ chmod +x ${ mconda } && \\ ./ ${ mconda } -b -p /opt/miniconda3 && \\ ln -s /opt/miniconda3/bin/conda /usr/bin/conda # metaWRAP dependencies installation ============================== /bin/bash <<EOF source /opt/miniconda3/etc/profile.d/conda.sh conda create -y -n metawrap-env python=2.7 conda activate metawrap-env conda config --add channels defaults conda config --add channels conda-forge conda config --add channels bioconda conda config --add channels ursky conda install --only-deps -c ursky metawrap-mg conda clean --all --yes EOF # metaWRAP from github ============================================ cd /opt git clone https://github.com/bxlab/metaWRAP.git cd / && rm -r /installs %runscript params = $@ /bin/bash <<EOF export PATH=/opt/metaWRAP/bin:$PATH source /opt/miniconda3/etc/profile.d/conda.sh conda activate metawrap-env metawrap $params EOF pip Install only the minimum python ( python3-dev ) from the distribution package manager and the equivalent for build-essential . The rest should be perhaps better done by pip . Some libraries might still be needed. Downloading packages and files multiple times. Package installation - apt, yum, etc... Even if you use --sandbox you might find that some commands do not behave the same way as when executed by the common routines sudo build... . Some of these problems are related to the shell interpreter which might be sh or bash ... Warning This nice file fetching trick will work interactively when you test but it will fail during the build wget -P bcftools/plugins https://raw.githubusercontent.com/freeseek/gtc2vcf/master/{gtc2vcf.{c,h},affy2vcf.c} It needs to be tricked a bit. /bin/bash -c 'wget -P bcftools/plugins https://raw.githubusercontent.com/freeseek/gtc2vcf/master/{gtc2vcf.{c,h},affy2vcf.c}' Now, if you find yourself repeatedly rebuilding your definition file... and you find that every time you need to re-download packages from the repositories... Some hosting services might slow you down or even block you upon repetitive downloads... 1 2 3 4 5 6 7 8 9 10 11 12 13 %post mkdir -p /tmp/apt echo \"Dir::Cache \" /tmp/apt \";\" > /etc/apt/apt.conf.d/singularity-cache.conf apt-get update && \\ apt-get --no-install-recommends -y install wget unzip git bwa samtools # the usual clean up rm -rf /var/lib/apt/lists/* ... # remove the /tmp/apt caching configuration rm /etc/apt/apt.conf.d/singularity-cache.conf Note Remember to remove these lines in the final recipe. note the --no-install-recommends which can save on installing unnecessary packages. It is rather popular option. Downloading large files The example bellow is from the installation instructions for https://github.com/freeseek/gtc2vcf. Here is the original code, which downloads the 871MB file and extracts it on the fly. Then some indexing is applied. wget -O- ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/human_g1k_v37.fasta.gz | \\ gzip -d > $HOME/GRCh37/human_g1k_v37.fasta samtools faidx $HOME/GRCh37/human_g1k_v37.fasta bwa index $HOME/GRCh37/human_g1k_v37.fasta The file is rather large for multiple downloads... we could rewrite a bit the lines like this and keep the original file during builds. %post export TMPD = /tmp/downloads mkdir -p $TMPD # Install the GRCh37 human genome reference ======================================= mkdir -p /data/GRCh37 && cd /data/GRCh37 wget -P $TMPD -c ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/human_g1k_v37.fasta.gz gunzip -c $TMPD /human_g1k_v37.fasta.gz > human_g1k_v37.fasta || true samtools faidx /data/GRCh37/human_g1k_v37.fasta bwa index /data/GRCh37/human_g1k_v37.fasta || true Note gunzip is returning non-zero exit code which signals an error and the Singularity build will stop. The not so nice solution is to apply the || true \"trick\" to ignore the error. Similar for the bwa tool. Warning The samtools and bwa are computationally intensive, memory demanding, and time demanding. This will conflict with some of the limitations of the free online building services. You might consider doing this outside the container and only copy the files (the uncompressed result is even larger) or better - as in the original instructions they will be installed in the user's $HOME directory. Have look for alternative advanced ideas - Image Mounts","title":"Goods and bads"},{"location":"goods_bads/#common-good-or-bad-practices-for-building-singularity-containers","text":"Disclaimer: these might not be the best solutions at all.","title":"Common good or bad practices for building Singularity containers"},{"location":"goods_bads/#where-to-compile-and-install-source-codes","text":"There are some, not so easy to see, complications. /tmp looks like good choice... The problem is that /tmp is mounted automatically even during the build process. This means that you will collide with leftovers from previous builds which might lead to rather unexpected results. We will use this problematic behavior in the next section for our advantage. $HOME points to /root during build, and it is also mounted at build time. Really bad place to compile! So... Where is a good place to compile and install? Here is an example scenario 1 2 3 4 5 6 7 8 9 10 11 12 13 %environment export PATH = /opt/tool-name/bin: ${ PATH } %post mkdir -p /installs && cd /installs git clone repository-link && cd tool-name ./configure --prefix = /opt/tool-name make && make install ... # Clean the installation folder (unless you want to keep it) rm -rf /installs dedicate a folder in the container's file structure fetch your installation files there (look how this might be improved for large files downloaded with wget ) install in /opt and adjust the $PATH or just allow the tool to mix with the system files.","title":"Where to compile and install source codes."},{"location":"goods_bads/#conda","text":"Conda causes some unexpected problems. During the build and and the commands in %runscrupt sections are run with /bin/sh which fails upon source /full_path_to/conda.sh which in turn fails conda activate my_environment . Her are two examples how to deal with the situation. docker://continuumio/miniconda3 container 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 Bootstrap : docker From : continuumio/miniconda3 %environment export LC_ALL = C %post export LC_ALL = C conda config --add channels defaults conda config --add channels conda-forge conda config --add channels bioconda conda config --add channels ursky conda create --name metawrap-env --channel ursky metawrap-mg = 1 .3.2 tbb = 2020 .2 conda clean --all --yes %runscript params = $@ /bin/bash <<EOF source /opt/conda/etc/profile.d/conda.sh conda activate metawrap-env metawrap $params EOF Ubuntu + conda 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 Bootstrap : docker From : ubuntu: 20.04 %labels Author pmitev@gmail.com %environment export LC_ALL = C export CONDA_ENVS_PATH = /opt/conda_envs export PATH = /opt/metaWRAP/bin: $PATH %post export DEBIAN_FRONTEND = noninteractive export LC_ALL = C export CONDA_ENVS_PATH = /opt/conda_envs && mkdir -p ${ CONDA_ENVS_PATH } apt-get update && apt-get -y install wget git mkdir /installs && cd /installs # Conda installation ============================================== mconda = \"Miniconda3-py38_4.9.2-Linux-x86_64.sh\" wget https://repo.anaconda.com/miniconda/ ${ mconda } && \\ chmod +x ${ mconda } && \\ ./ ${ mconda } -b -p /opt/miniconda3 && \\ ln -s /opt/miniconda3/bin/conda /usr/bin/conda # metaWRAP dependencies installation ============================== /bin/bash <<EOF source /opt/miniconda3/etc/profile.d/conda.sh conda create -y -n metawrap-env python=2.7 conda activate metawrap-env conda config --add channels defaults conda config --add channels conda-forge conda config --add channels bioconda conda config --add channels ursky conda install --only-deps -c ursky metawrap-mg conda clean --all --yes EOF # metaWRAP from github ============================================ cd /opt git clone https://github.com/bxlab/metaWRAP.git cd / && rm -r /installs %runscript params = $@ /bin/bash <<EOF export PATH=/opt/metaWRAP/bin:$PATH source /opt/miniconda3/etc/profile.d/conda.sh conda activate metawrap-env metawrap $params EOF","title":"Conda"},{"location":"goods_bads/#pip","text":"Install only the minimum python ( python3-dev ) from the distribution package manager and the equivalent for build-essential . The rest should be perhaps better done by pip . Some libraries might still be needed.","title":"pip"},{"location":"goods_bads/#downloading-packages-and-files-multiple-times","text":"","title":"Downloading packages and files multiple times."},{"location":"goods_bads/#package-installation-apt-yum-etc","text":"Even if you use --sandbox you might find that some commands do not behave the same way as when executed by the common routines sudo build... . Some of these problems are related to the shell interpreter which might be sh or bash ... Warning This nice file fetching trick will work interactively when you test but it will fail during the build wget -P bcftools/plugins https://raw.githubusercontent.com/freeseek/gtc2vcf/master/{gtc2vcf.{c,h},affy2vcf.c} It needs to be tricked a bit. /bin/bash -c 'wget -P bcftools/plugins https://raw.githubusercontent.com/freeseek/gtc2vcf/master/{gtc2vcf.{c,h},affy2vcf.c}' Now, if you find yourself repeatedly rebuilding your definition file... and you find that every time you need to re-download packages from the repositories... Some hosting services might slow you down or even block you upon repetitive downloads... 1 2 3 4 5 6 7 8 9 10 11 12 13 %post mkdir -p /tmp/apt echo \"Dir::Cache \" /tmp/apt \";\" > /etc/apt/apt.conf.d/singularity-cache.conf apt-get update && \\ apt-get --no-install-recommends -y install wget unzip git bwa samtools # the usual clean up rm -rf /var/lib/apt/lists/* ... # remove the /tmp/apt caching configuration rm /etc/apt/apt.conf.d/singularity-cache.conf Note Remember to remove these lines in the final recipe. note the --no-install-recommends which can save on installing unnecessary packages. It is rather popular option.","title":"Package installation - apt, yum, etc..."},{"location":"goods_bads/#downloading-large-files","text":"The example bellow is from the installation instructions for https://github.com/freeseek/gtc2vcf. Here is the original code, which downloads the 871MB file and extracts it on the fly. Then some indexing is applied. wget -O- ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/human_g1k_v37.fasta.gz | \\ gzip -d > $HOME/GRCh37/human_g1k_v37.fasta samtools faidx $HOME/GRCh37/human_g1k_v37.fasta bwa index $HOME/GRCh37/human_g1k_v37.fasta The file is rather large for multiple downloads... we could rewrite a bit the lines like this and keep the original file during builds. %post export TMPD = /tmp/downloads mkdir -p $TMPD # Install the GRCh37 human genome reference ======================================= mkdir -p /data/GRCh37 && cd /data/GRCh37 wget -P $TMPD -c ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/human_g1k_v37.fasta.gz gunzip -c $TMPD /human_g1k_v37.fasta.gz > human_g1k_v37.fasta || true samtools faidx /data/GRCh37/human_g1k_v37.fasta bwa index /data/GRCh37/human_g1k_v37.fasta || true Note gunzip is returning non-zero exit code which signals an error and the Singularity build will stop. The not so nice solution is to apply the || true \"trick\" to ignore the error. Similar for the bwa tool. Warning The samtools and bwa are computationally intensive, memory demanding, and time demanding. This will conflict with some of the limitations of the free online building services. You might consider doing this outside the container and only copy the files (the uncompressed result is even larger) or better - as in the original instructions they will be installed in the user's $HOME directory. Have look for alternative advanced ideas - Image Mounts","title":"Downloading large files"},{"location":"indirect-call/","text":"Indirect executable calls The common approach for Singularity is to have single entry point defined by %runscript or by %app . This is not so convenient for daily use... So, here is a minimal example on how to implement a well known trick to use single executable for multiple commands (see for example the BusBox project). Bootstrap : docker From : ubuntu: 20.04 %environment export LC_ALL = C %post export LC_ALL = C export DEBIAN_FRONTEND = noninteractive mkdir -p /tmp/apt echo \"Dir::Cache /tmp/apt;\" > /etc/apt/apt.conf.d/singularity-cache.conf apt-get update && \\ apt-get --no-install-recommends -y install wget unzip git bwa samtools bcftools bowtie %runscript if command -v $SINGULARITY_NAME & > /dev/null ; then exec $SINGULARITY_NAME \" $@ \" else echo \"# ERROR !!! Command $SINGULARITY_NAME not found in the container\" fi Let's build the recipe and make soft links to the executables in the image: sudo singularity build samtools.sif Singularity.samtools # make bin folder mkdir -p bin # Extraxt the executable names from the packages of interest SIMG = samtools.sif bins = $( singularity exec ${ SIMG } dpkg -L bwa samtools bcftools bowtie | grep /bin/ ) # Make softlinks pointing to the Singularity image for i in $bins ; do echo $i ; ln -s ../ ${ SIMG } bin/ ${ i ##*/ } ; done # Check what is the content of bin [ 09 :11:36 ] > ls -l bin total 0 lrwxrwxrwx 1 user user 15 Apr 4 09 :09 ace2sam -> ../samtools.sif lrwxrwxrwx 1 user user 15 Apr 4 09 :09 bcftools -> ../samtools.sif lrwxrwxrwx 1 user user 15 Apr 4 09 :09 blast2sam.pl -> ../samtools.sif lrwxrwxrwx 1 user user 15 Apr 4 09 :09 bowtie -> ../samtools.sif lrwxrwxrwx 1 user user 15 Apr 4 09 :09 bowtie2sam.pl -> ../samtools.sif lrwxrwxrwx 1 user user 15 Apr 4 09 :09 bowtie-align-l -> ../samtools.sif lrwxrwxrwx 1 user user 15 Apr 4 09 :09 bowtie-align-l-debug -> ../samtools.sif lrwxrwxrwx 1 user user 15 Apr 4 09 :09 bowtie-align-s -> ../samtools.sif lrwxrwxrwx 1 user user 15 Apr 4 09 :09 bowtie-align-s-debug -> ../samtools.sif lrwxrwxrwx 1 user user 15 Apr 4 09 :09 bowtie-build -> ../samtools.sif ... Let's test the tools (you can add the bin folder in $PATH if you want...) ./bin/bwa /usr/bin/bwa Program: bwa ( alignment via Burrows-Wheeler transformation ) Version: 0 .7.17-r1188 Contact: Heng Li <lh3@sanger.ac.uk> Usage: bwa <command> [ options ] Command: index index sequences in the FASTA format mem BWA-MEM algorithm fastmap identify super-maximal exact matches pemerge merge overlapping paired ends ( EXPERIMENTAL ) aln gapped/ungapped alignment samse generate alignment ( single ended ) sampe generate alignment ( paired ended ) bwasw BWA-SW for long queries ... ./bin/samtools --help /usr/bin/samtools Program: samtools ( Tools for alignments in the SAM format ) Version: 1 .10 ( using htslib 1 .10.2-3 ) Usage: samtools <command> [ options ] Commands: -- Indexing dict create a sequence dictionary file faidx index/extract FASTA fqidx index/extract FASTQ index index alignment ... ./bin/bcftools --help /usr/bin/bcftools Program: bcftools ( Tools for variant calling and manipulating VCFs and BCFs ) Version: 1 .10.2 ( using htslib 1 .10.2-3 ) Usage: bcftools [ --version | --version-only ] [ --help ] <command> <argument> Commands: -- Indexing index index VCF/BCF files ... We can add other tools that we want from the image... # Make soft link for date ln -s ../samtools.sif bin/date # Running date from the image ./bin/date /usr/bin/date Mon Apr 4 07 :23:16 Europe 2022 # Running date from the container date Mon 04 Apr 2022 09 :24:12 AM CEST Note the different time zones ;-)","title":"Indirect calls"},{"location":"indirect-call/#indirect-executable-calls","text":"The common approach for Singularity is to have single entry point defined by %runscript or by %app . This is not so convenient for daily use... So, here is a minimal example on how to implement a well known trick to use single executable for multiple commands (see for example the BusBox project). Bootstrap : docker From : ubuntu: 20.04 %environment export LC_ALL = C %post export LC_ALL = C export DEBIAN_FRONTEND = noninteractive mkdir -p /tmp/apt echo \"Dir::Cache /tmp/apt;\" > /etc/apt/apt.conf.d/singularity-cache.conf apt-get update && \\ apt-get --no-install-recommends -y install wget unzip git bwa samtools bcftools bowtie %runscript if command -v $SINGULARITY_NAME & > /dev/null ; then exec $SINGULARITY_NAME \" $@ \" else echo \"# ERROR !!! Command $SINGULARITY_NAME not found in the container\" fi Let's build the recipe and make soft links to the executables in the image: sudo singularity build samtools.sif Singularity.samtools # make bin folder mkdir -p bin # Extraxt the executable names from the packages of interest SIMG = samtools.sif bins = $( singularity exec ${ SIMG } dpkg -L bwa samtools bcftools bowtie | grep /bin/ ) # Make softlinks pointing to the Singularity image for i in $bins ; do echo $i ; ln -s ../ ${ SIMG } bin/ ${ i ##*/ } ; done # Check what is the content of bin [ 09 :11:36 ] > ls -l bin total 0 lrwxrwxrwx 1 user user 15 Apr 4 09 :09 ace2sam -> ../samtools.sif lrwxrwxrwx 1 user user 15 Apr 4 09 :09 bcftools -> ../samtools.sif lrwxrwxrwx 1 user user 15 Apr 4 09 :09 blast2sam.pl -> ../samtools.sif lrwxrwxrwx 1 user user 15 Apr 4 09 :09 bowtie -> ../samtools.sif lrwxrwxrwx 1 user user 15 Apr 4 09 :09 bowtie2sam.pl -> ../samtools.sif lrwxrwxrwx 1 user user 15 Apr 4 09 :09 bowtie-align-l -> ../samtools.sif lrwxrwxrwx 1 user user 15 Apr 4 09 :09 bowtie-align-l-debug -> ../samtools.sif lrwxrwxrwx 1 user user 15 Apr 4 09 :09 bowtie-align-s -> ../samtools.sif lrwxrwxrwx 1 user user 15 Apr 4 09 :09 bowtie-align-s-debug -> ../samtools.sif lrwxrwxrwx 1 user user 15 Apr 4 09 :09 bowtie-build -> ../samtools.sif ... Let's test the tools (you can add the bin folder in $PATH if you want...) ./bin/bwa /usr/bin/bwa Program: bwa ( alignment via Burrows-Wheeler transformation ) Version: 0 .7.17-r1188 Contact: Heng Li <lh3@sanger.ac.uk> Usage: bwa <command> [ options ] Command: index index sequences in the FASTA format mem BWA-MEM algorithm fastmap identify super-maximal exact matches pemerge merge overlapping paired ends ( EXPERIMENTAL ) aln gapped/ungapped alignment samse generate alignment ( single ended ) sampe generate alignment ( paired ended ) bwasw BWA-SW for long queries ... ./bin/samtools --help /usr/bin/samtools Program: samtools ( Tools for alignments in the SAM format ) Version: 1 .10 ( using htslib 1 .10.2-3 ) Usage: samtools <command> [ options ] Commands: -- Indexing dict create a sequence dictionary file faidx index/extract FASTA fqidx index/extract FASTQ index index alignment ... ./bin/bcftools --help /usr/bin/bcftools Program: bcftools ( Tools for variant calling and manipulating VCFs and BCFs ) Version: 1 .10.2 ( using htslib 1 .10.2-3 ) Usage: bcftools [ --version | --version-only ] [ --help ] <command> <argument> Commands: -- Indexing index index VCF/BCF files ... We can add other tools that we want from the image... # Make soft link for date ln -s ../samtools.sif bin/date # Running date from the image ./bin/date /usr/bin/date Mon Apr 4 07 :23:16 Europe 2022 # Running date from the container date Mon 04 Apr 2022 09 :24:12 AM CEST Note the different time zones ;-)","title":"Indirect executable calls"},{"location":"inspect/","text":"Inspecting the metadata of the container Online documentation $ singularity inspect [ inspect options... ] <image path> Inspect will show you labels, environment variables, apps and scripts associated with the image determined by the flags you pass. Let's inspect our containers. -d - show the Singularity recipe file that was used to generate the image $ singularity inspect -d lolcow.sif BootStrap: docker From: ubuntu:16.04 %post apt-get -y update apt-get -y install fortune cowsay lolcat %environment export LC_ALL=C export PATH=/usr/games:$PATH %runscript fortune | cowsay | lolcat -e - show the environment settings for the image $ singularity inspect -e lolcow.sif === /.singularity.d/env/10-docker2singularity.sh === #!/bin/sh export PATH=\"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\" === /.singularity.d/env/90-environment.sh === #!/bin/sh # Custom environment shell code should follow export LC_ALL=C export PATH=/usr/games:$PATH -r - show the runscript for the image $ singularity inspect -r lolcow.sif #!/bin/sh fortune | cowsay | lolcat Note Do not assume that the content of the script will give you all the information. Just test with an image you have downloaded from docker:// docker:// singularity inspect -d wttr.sif bootstrap: docker from: dctrud/wttr Check the runscript.","title":"Inspect containers"},{"location":"inspect/#inspecting-the-metadata-of-the-container","text":"Online documentation $ singularity inspect [ inspect options... ] <image path> Inspect will show you labels, environment variables, apps and scripts associated with the image determined by the flags you pass.","title":"Inspecting the metadata of the container"},{"location":"inspect/#lets-inspect-our-containers","text":"-d - show the Singularity recipe file that was used to generate the image $ singularity inspect -d lolcow.sif BootStrap: docker From: ubuntu:16.04 %post apt-get -y update apt-get -y install fortune cowsay lolcat %environment export LC_ALL=C export PATH=/usr/games:$PATH %runscript fortune | cowsay | lolcat -e - show the environment settings for the image $ singularity inspect -e lolcow.sif === /.singularity.d/env/10-docker2singularity.sh === #!/bin/sh export PATH=\"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\" === /.singularity.d/env/90-environment.sh === #!/bin/sh # Custom environment shell code should follow export LC_ALL=C export PATH=/usr/games:$PATH -r - show the runscript for the image $ singularity inspect -r lolcow.sif #!/bin/sh fortune | cowsay | lolcat Note Do not assume that the content of the script will give you all the information. Just test with an image you have downloaded from docker:// docker:// singularity inspect -d wttr.sif bootstrap: docker from: dctrud/wttr Check the runscript.","title":"Let's inspect our containers."},{"location":"installation/","text":"Information Singularity project is officially moving into the Linux Foundation . As part of this move, and to differentiate from the other like-named projects and commercial products, we will be renaming the project to \" Apptainer \". source Disclaimer During the last 2021 year, the installation instructions changed more frequent than the workshop was given... Always check with the original instructions on how to install Singularity/Apptainer. For the purpose of this workshop, we will try to adapt to the the new free derivative as soon it becomes stable https://github.com/apptainer/apptainer . Until then the content bellow will remain unchanged, to avoid unnecessary modifications. If you have already installed Singularity version, newer than 3.7, it should be sufficient for the workshop. Perhaps the easiest way to install Singularity under Linux Currently, for supported Ubuntu and CentOS distributions, it is also possible to install Singuarity via the system package manager link # Ubuntu 20.04 wget https://github.com/sylabs/singularity/releases/download/v3.9.7/singularity-ce_3.9.7-focal_amd64.deb sudo apt install ./singularity-ce_3.9.7-focal_amd64.deb Installation Detailed and well explained installation instructions at: https://sylabs.io/guides/latest/admin-guide/installation.html#installation-on-linux Installation on Windows or Mac https://sylabs.io/guides/latest/admin-guide/installation.html#installation-on-windows-or-mac (PM) I have successfully installed Singularity under WSL2 , but can't guarantee that it will work in all cases. TL;DR For Ubuntu (Debian based) Linux distributions. # Install system dependencies sudo apt-get update && sudo apt-get install -y \\ build-essential libssl-dev uuid-dev libgpgme11-dev \\ squashfs-tools libseccomp-dev wget pkg-config git cryptsetup # Install Go export VERSION = 1 .17.6 OS = linux ARCH = amd64 && # Replace the values as needed \\ wget https://dl.google.com/go/go $VERSION . $OS - $ARCH .tar.gz && # Downloads the required Go package \\ sudo tar -C /usr/local -xzvf go $VERSION . $OS - $ARCH .tar.gz && # Extracts the archive \\ rm go $VERSION . $OS - $ARCH .tar.gz # Deletes the ``tar`` file echo 'export GOPATH=${HOME}/go' >> ~/.bashrc && \\ echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \\ source ~/.bashrc # Clone Singularity, checkout from a release, compile end install export VERSION = v3.9.2 && \\ git clone https://github.com/sylabs/singularity.git && \\ cd singularity && git checkout $VERSION && \\ ./mconfig && cd ./builddir && make -j 4 && sudo make install Note The code development is very dynamic, please always check with the original instructions if in doubt. Test the installation https://sylabs.io/guides/latest/admin-guide/installation.html#testing-checking-the-build-configuration TL;DR singularity Usage: singularity [ global options... ] <command> Available Commands: build Build a Singularity image cache Manage the local cache capability Manage Linux capabilities for users and groups config Manage various singularity configuration ( root user only ) delete Deletes requested image from the library exec Run a command within a container inspect Show metadata for an image instance Manage containers running as services key Manage OpenPGP keys oci Manage OCI containers plugin Manage Singularity plugins pull Pull an image from a URI push Upload image to the provided URI remote Manage singularity remote endpoints, keyservers and OCI/Docker registry credentials run Run the user-defined default command within a container run-help Show the user-defined help for an image search Search a Container Library for images shell Run a shell within a container sif siftool is a program for Singularity Image Format ( SIF ) file manipulation sign Attach digital signature ( s ) to an image test Run the user-defined tests within a container verify Verify cryptographic signatures attached to an image version Show the version for Singularity Run 'singularity --help' for more detailed usage information. Check the configuration singularity buildcfg PACKAGE_NAME = singularity PACKAGE_VERSION = 3 .8.4 BUILDDIR = /root/singularity/builddir PREFIX = /usr/local EXECPREFIX = /usr/local BINDIR = /usr/local/bin SBINDIR = /usr/local/sbin LIBEXECDIR = /usr/local/libexec DATAROOTDIR = /usr/local/share DATADIR = /usr/local/share SYSCONFDIR = /usr/local/etc SHAREDSTATEDIR = /usr/local/com LOCALSTATEDIR = /usr/local/var RUNSTATEDIR = /usr/local/var/run INCLUDEDIR = /usr/local/include DOCDIR = /usr/local/share/doc/singularity INFODIR = /usr/local/share/info LIBDIR = /usr/local/lib LOCALEDIR = /usr/local/share/locale MANDIR = /usr/local/share/man SINGULARITY_CONFDIR = /usr/local/etc/singularity SESSIONDIR = /usr/local/var/singularity/mnt/session PLUGIN_ROOTDIR = /usr/local/libexec/singularity/plugin SINGULARITY_CONF_FILE = /usr/local/etc/singularity/singularity.conf SINGULARITY_SUID_INSTALL = 1 Note There is a possibility to install Singularity as user , providing that the requirements are satisfied. Make sure you have go or install it as user as well.","title":"How to install"},{"location":"installation/#installation","text":"Detailed and well explained installation instructions at: https://sylabs.io/guides/latest/admin-guide/installation.html#installation-on-linux Installation on Windows or Mac https://sylabs.io/guides/latest/admin-guide/installation.html#installation-on-windows-or-mac (PM) I have successfully installed Singularity under WSL2 , but can't guarantee that it will work in all cases.","title":"Installation"},{"location":"installation/#tldr","text":"For Ubuntu (Debian based) Linux distributions. # Install system dependencies sudo apt-get update && sudo apt-get install -y \\ build-essential libssl-dev uuid-dev libgpgme11-dev \\ squashfs-tools libseccomp-dev wget pkg-config git cryptsetup # Install Go export VERSION = 1 .17.6 OS = linux ARCH = amd64 && # Replace the values as needed \\ wget https://dl.google.com/go/go $VERSION . $OS - $ARCH .tar.gz && # Downloads the required Go package \\ sudo tar -C /usr/local -xzvf go $VERSION . $OS - $ARCH .tar.gz && # Extracts the archive \\ rm go $VERSION . $OS - $ARCH .tar.gz # Deletes the ``tar`` file echo 'export GOPATH=${HOME}/go' >> ~/.bashrc && \\ echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \\ source ~/.bashrc # Clone Singularity, checkout from a release, compile end install export VERSION = v3.9.2 && \\ git clone https://github.com/sylabs/singularity.git && \\ cd singularity && git checkout $VERSION && \\ ./mconfig && cd ./builddir && make -j 4 && sudo make install Note The code development is very dynamic, please always check with the original instructions if in doubt.","title":"TL;DR"},{"location":"installation/#test-the-installation","text":"https://sylabs.io/guides/latest/admin-guide/installation.html#testing-checking-the-build-configuration TL;DR singularity Usage: singularity [ global options... ] <command> Available Commands: build Build a Singularity image cache Manage the local cache capability Manage Linux capabilities for users and groups config Manage various singularity configuration ( root user only ) delete Deletes requested image from the library exec Run a command within a container inspect Show metadata for an image instance Manage containers running as services key Manage OpenPGP keys oci Manage OCI containers plugin Manage Singularity plugins pull Pull an image from a URI push Upload image to the provided URI remote Manage singularity remote endpoints, keyservers and OCI/Docker registry credentials run Run the user-defined default command within a container run-help Show the user-defined help for an image search Search a Container Library for images shell Run a shell within a container sif siftool is a program for Singularity Image Format ( SIF ) file manipulation sign Attach digital signature ( s ) to an image test Run the user-defined tests within a container verify Verify cryptographic signatures attached to an image version Show the version for Singularity Run 'singularity --help' for more detailed usage information.","title":"Test the installation"},{"location":"installation/#check-the-configuration","text":"singularity buildcfg PACKAGE_NAME = singularity PACKAGE_VERSION = 3 .8.4 BUILDDIR = /root/singularity/builddir PREFIX = /usr/local EXECPREFIX = /usr/local BINDIR = /usr/local/bin SBINDIR = /usr/local/sbin LIBEXECDIR = /usr/local/libexec DATAROOTDIR = /usr/local/share DATADIR = /usr/local/share SYSCONFDIR = /usr/local/etc SHAREDSTATEDIR = /usr/local/com LOCALSTATEDIR = /usr/local/var RUNSTATEDIR = /usr/local/var/run INCLUDEDIR = /usr/local/include DOCDIR = /usr/local/share/doc/singularity INFODIR = /usr/local/share/info LIBDIR = /usr/local/lib LOCALEDIR = /usr/local/share/locale MANDIR = /usr/local/share/man SINGULARITY_CONFDIR = /usr/local/etc/singularity SESSIONDIR = /usr/local/var/singularity/mnt/session PLUGIN_ROOTDIR = /usr/local/libexec/singularity/plugin SINGULARITY_CONF_FILE = /usr/local/etc/singularity/singularity.conf SINGULARITY_SUID_INSTALL = 1 Note There is a possibility to install Singularity as user , providing that the requirements are satisfied. Make sure you have go or install it as user as well.","title":"Check the configuration"},{"location":"mpi_applications/","text":"Singularity and MPI applications The Singularity documentation is excellent starting point - link The C3SE Singularity has really nice summary that is included bellow - source Running singularity with MPI across multiple nodes There are four main components involved in a containerised MPI-based application: 1.The executable MPI program (e.g. a.out) 2.The MPI library 3.MPI runtime, e.g. mpirun 4.Communication channel, e.g. SSH server Depending on how to containerise those elements, there are two general approaches to running a containerised application across a multi-node cluster: 1.Packaging the MPI program and the MPI library inside the container, but keeping the MPI runtime outside on the host 2.Packaging the MPI runtime also inside the container leaving only the communication channel on the host Host-based MPI runtime In this approach, the mpirun command runs on the host: $ mpirun singularity run myImage.sif myMPI_program mpirun does, among other things, the following: * Spawns the ORTE daemon on the compute nodes * Launches the MPI program on the nodes * Manages the communication among the MPI ranks This fits perfectly with the regular workflow of submitting jobs on the HPC clusters, and is, therefore, the recommended approach . There is one thing to keep in mind however: The MPI runtime on the host needs to be able to communicate with the MPI library inside the container; therefore, i) there must be the same implementation of the MPI standard (e.g. OpenMPI) inside the container, and, ii) the version of the two MPI libraries should be as close to one another as possible to prevent unpredictable behaviour (ideally the exact same version). Image-based MPI runtime In this approach, the MPI launcher is called from within the container; therefore, it can even run on a host system without an MPI installation (your challenge would be to find one!): $ singularity run myImage.sif mpirun myMPI_program Everything works well on a single node. There's a problem though: as soon as the launcher tries to spawn into the second node, the ORTED process crashes. The reason is it tries to launch the MPI runtime on the host and not inside the container. The solution is to have a launch agent do it inside the container. With OpenMPI, that would be: $ singularity run myImage.sif mpirun --launch-agent 'singularity run myImage.sif orted' myMPI_program Example - running conteinerized gromacs in parallel Here is a simple setup to build and test simple Singularity container with Gromacs with binaries for OpenMPI parallelization provided with the Ubuntu 20.04 distribution. Bootstrap : docker From : ubuntu: 20.04 %setup %files %environment export LC_ALL = C %post export LC_ALL = C export DEBIAN_FRONTEND = noninteractive apt-get update && apt-get -y dist-upgrade && apt-get install -y git wget gawk cmake build-essential libopenmpi-dev openssh-client slurm-client gromacs-openmpi %runscript /usr/bin/mdrun_mpi \" $@ \" At the time of writing this page, the recipe will install GROMACS - mdrun_mpi, 2020.1-Ubuntu-2020.1-1 mpirun (Open MPI) 4.0.3 Note Keep in mind that the binaries provided by the package manager are not optimized for the CPU and you could read such message in the output: Compiled SIMD: SSE2, but for this host/run AVX2_256 might be better (see log). The current CPU can measure timings more accurately than the code in mdrun_mpi was configured to use. This might affect your simulation speed as accurate timings are needed for load-balancing. Please consider rebuilding mdrun_mpi with the GMX_USE_RDTSCP=ON CMake option. Reading file benchMEM.tpr, VERSION 4.6.3-dev-20130701-6e3ae9e (single precision) Note: file tpx version 83, software tpx version 119 We can use A free GROMACS benchmark set to perform tests with this set . Running on single node Image-based MPI runtime # Run shell in the container $ singularity shell Gromacs-openmpi20.sif # 2 MPI processes, 4 OpenMP threads per MPI process Singularity> mpirun -n 2 mdrun_mpi -ntomp 4 -s benchMEM.tpr -nsteps 10000 -resethway Host-based MPI runtime On my host which runs Ubuntu 20.04 the OpenMPI version is identical. On other machines you need to make sure to run compatible version. On HPC clusters this should be module which provides OpenMPI compiled with gcc. $ mpirun -n 2 singularity exec Gromacs-openmpi20.sif /usr/bin/mdrun_mpi -ntomp 4 -s benchMEM.tpr -nsteps 10000 -resethway # or relying on the runscript $ mpirun -n 2 Gromacs-openmpi20.sif -ntomp 4 -s benchMEM.tpr -nsteps 10000 -resethway Note: At present, there is a incompatibility in the OpenMPI version that causes \"Segmentation fault\" when trying to run it on Rackham@UPPMAX. Bootsraping from Ubntu 18.04 will install GROMACS - mdrun_mpi, 2018.1 and mpirun (Open MPI) 2.1.1 which \"resolves\" the problem and reveals other... Image-based MPI runtime: Run 2 MPI processes, 20 OpenMP threads per MPI process scontrol show hostname $SLURM_NODELIST > host singularity exec -B /etc/slurm:/etc/slurm-llnl -B /run/munge Gromacs-openmpi18.sif mpirun -n 2 -d -mca plm_base_verbose 10 --launch-agent 'singularity exec Gromacs-openmpi18.sif orted' /usr/bin/mdrun_mpi -ntomp 20 -s benchMEM.tpr -nsteps 10000 -resethway Use -d -mca plm_base_verbose 10 to get debugging information from mpirun . The slurm client that provides srun in Ubuntu 18.04 is too old to read the current slurm configuration on Rackham@UPPMAX. # Ubuntu 18.04 SLURM error # ============================ ... srun : error : _parse_next_key : Parsing error at unrecognized key : SlurmctldHost srun : error : Parse error in file / etc / slurm - llnl / slurm . conf line 5 : \"SlurmctldHost=rackham-q\" srun : error : _parse_next_key : Parsing error at unrecognized key : CredType srun : error : Parse error in file / etc / slurm - llnl / slurm . conf line 7 : \"CredType=cred/munge\" srun : fatal : Unable to process configuration file ... # Ubuntu 20.04 SLURM passes but gromacs+mpi binaries fail on Rackham # ===================================================================== ... [ r483 . uppmax . uu . se : 25362 ] [[ 34501 , 0 ], 0 ] plm : slurm : final top - level argv : srun -- ntasks - per - node = 1 -- kill - on - bad - exit -- nodes = 1 -- nodelist = r484 -- ntasks = 1 / usr / bin / singularity exec Gromacs - apt . sif / usr / bin / orted - mca ess \"slurm\" - mca ess _base_jobid \"2261057536\" - mca ess_base_vpid \"1\" - mca ess_base_num_procs \"2\" - mca orte_node_regex \"r[3:483-484]@0(2)\" - mca orte_hnp_uri \"2261057536.0;tcp://172.18.10.240,10.1.10.234,10.0.10.234:44203\" - mca plm_base_verbose \"10\" - mca - d \"- display - allocation \" [ r483 . uppmax . uu . se : 25362 ] [[ 34501 , 0 ], 0 ] complete_setup on job [ 34501 , 1 ] Data for JOB [ 34501 , 1 ] offset 0 Total slots allocated 2 2261057536.0 ; tcp : //172.18.10.240,10.1.10.234,10.0.10.234:44203 ... Reading file benchMEM . tpr , VERSION 4.6.3 - dev -20130701 -6e3 ae9e ( single precision ) Note : file tpx version 83 , software tpx version 119 [ r484 : 19455 ] *** Process received signal *** [ r484 : 19455 ] Signal : Segmentation fault ( 11 ) ... Attempting to start the jobs manually requires -mca plm rsh to skip SLURM and use rsh/ssh to start the processes on all allocated nodes and -B /etc/ssh to pick up the \"Host authentication\" setup. Here are the problems: Rackham@UPPMAX uses \" Host based authentication \" which does not work in the Singularity container, because the container runs in the user space and can not get/read the private host key... One can use passwordless user key for the authentication ( Note: passwordless ssh keys are not allowed on Rackham ) after adding all nodes public host keys/signatures ( the signatures that you are asked to accept when connecting for the first time to a machine ). Host-based MPI runtime: Run 2 MPI processes, 20 OpenMP threads per MPI process (explicitly specified on the mpirun command line) #!/bin/bash -l #SBATCH -J test #SBATCH -t 00:15:00 #SBATCH -p devel -N 2 -n 2 #SBATCH --cpus-per-task 20 #SBATCH -A project module load gcc/7.2.0 openmpi/2.1.1 mpirun -n 2 Gromacs-openmpi18.sif -ntomp 20 -s benchMEM.tpr -nsteps 10000 -resethway # or # mpirun -n 2 singularity exec Gromacs-openmpi18.sif /usr/bin/mdrun_mpi -ntomp 20 -s benchMEM.tpr -nsteps 10000 -resethway Compiling Gromacs Note: the recipe bellow uses the OpenMPI from the distribution that triggers \"Segmentation fault\" problems when running the container on Rackham@UPPMAX. recipe Bootstrap : docker From : ubuntu: 20.04 %setup %files %environment export LC_ALL = C %post export LC_ALL = C export DEBIAN_FRONTEND = noninteractive export NCPU = $( grep -c ^processor /proc/cpuinfo ) apt-get update && apt-get -y dist-upgrade && apt-get install -y git wget gawk cmake build-essential libopenmpi-dev openssh-client mkdir -p installs # Gromacs mkdir -p /tmp/downloads && cd /tmp/downloads test -f gromacs-2021.2.tar.gz || wget https://ftp.gromacs.org/gromacs/gromacs-2021.2.tar.gz tar xf gromacs-2021.2.tar.gz -C /installs cd /installs/gromacs-2021.2 mkdir build-normal && cd build-normal cmake .. -DCMAKE_INSTALL_PREFIX = /opt/gromacs-2021.2 -DGMX_GPU = OFF -DGMX_MPI = ON -DGMX_THREAD_MPI = ON -DGMX_BUILD_OWN_FFTW = ON -DGMX_DOUBLE = OFF -DGM X_PREFER_STATIC_LIBS = ON -DBUILD_SHARED_LIBS = ON -DCMAKE_BUILD_TYPE = RELEASE make -j $NCPU && make install cd /installs/gromacs-2021.2 mkdir build-mdrun-only && cd build-mdrun-only cmake .. -DCMAKE_INSTALL_PREFIX = /opt/gromacs-2021.2 -DGMX_GPU = OFF -DGMX_MPI = ON -DGMX_THREAD_MPI = ON -DGMX_BUILD_OWN_FFTW = ON -DGMX_DOUBLE = OFF -DGM X_PREFER_STATIC_LIBS = ON -DBUILD_SHARED_LIBS = ON -DCMAKE_BUILD_TYPE = RELEASE -DGMX_BUILD_MDRUN_ONLY = ON make -j $NCPU && make install cd / rm -r /installs rm /etc/apt/apt.conf.d/singularity-cache.conf %runscript #!/bin/bash source /opt/gromacs-2021.2/bin/GMXRC exec gmx_mpi \" $@ \" GROMACS reminds you: \"Statistics: The only science that enables different experts using the same figures to draw different conclusions.\" (Evan Esar)","title":"Singularity and MPI"},{"location":"mpi_applications/#singularity-and-mpi-applications","text":"The Singularity documentation is excellent starting point - link The C3SE Singularity has really nice summary that is included bellow - source","title":"Singularity and MPI applications"},{"location":"mpi_applications/#running-singularity-with-mpi-across-multiple-nodes","text":"There are four main components involved in a containerised MPI-based application: 1.The executable MPI program (e.g. a.out) 2.The MPI library 3.MPI runtime, e.g. mpirun 4.Communication channel, e.g. SSH server Depending on how to containerise those elements, there are two general approaches to running a containerised application across a multi-node cluster: 1.Packaging the MPI program and the MPI library inside the container, but keeping the MPI runtime outside on the host 2.Packaging the MPI runtime also inside the container leaving only the communication channel on the host","title":"Running singularity with MPI across multiple nodes"},{"location":"mpi_applications/#host-based-mpi-runtime","text":"In this approach, the mpirun command runs on the host: $ mpirun singularity run myImage.sif myMPI_program mpirun does, among other things, the following: * Spawns the ORTE daemon on the compute nodes * Launches the MPI program on the nodes * Manages the communication among the MPI ranks This fits perfectly with the regular workflow of submitting jobs on the HPC clusters, and is, therefore, the recommended approach . There is one thing to keep in mind however: The MPI runtime on the host needs to be able to communicate with the MPI library inside the container; therefore, i) there must be the same implementation of the MPI standard (e.g. OpenMPI) inside the container, and, ii) the version of the two MPI libraries should be as close to one another as possible to prevent unpredictable behaviour (ideally the exact same version).","title":"Host-based MPI runtime"},{"location":"mpi_applications/#image-based-mpi-runtime","text":"In this approach, the MPI launcher is called from within the container; therefore, it can even run on a host system without an MPI installation (your challenge would be to find one!): $ singularity run myImage.sif mpirun myMPI_program Everything works well on a single node. There's a problem though: as soon as the launcher tries to spawn into the second node, the ORTED process crashes. The reason is it tries to launch the MPI runtime on the host and not inside the container. The solution is to have a launch agent do it inside the container. With OpenMPI, that would be: $ singularity run myImage.sif mpirun --launch-agent 'singularity run myImage.sif orted' myMPI_program","title":"Image-based MPI runtime"},{"location":"mpi_applications/#example-running-conteinerized-gromacs-in-parallel","text":"Here is a simple setup to build and test simple Singularity container with Gromacs with binaries for OpenMPI parallelization provided with the Ubuntu 20.04 distribution. Bootstrap : docker From : ubuntu: 20.04 %setup %files %environment export LC_ALL = C %post export LC_ALL = C export DEBIAN_FRONTEND = noninteractive apt-get update && apt-get -y dist-upgrade && apt-get install -y git wget gawk cmake build-essential libopenmpi-dev openssh-client slurm-client gromacs-openmpi %runscript /usr/bin/mdrun_mpi \" $@ \" At the time of writing this page, the recipe will install GROMACS - mdrun_mpi, 2020.1-Ubuntu-2020.1-1 mpirun (Open MPI) 4.0.3 Note Keep in mind that the binaries provided by the package manager are not optimized for the CPU and you could read such message in the output: Compiled SIMD: SSE2, but for this host/run AVX2_256 might be better (see log). The current CPU can measure timings more accurately than the code in mdrun_mpi was configured to use. This might affect your simulation speed as accurate timings are needed for load-balancing. Please consider rebuilding mdrun_mpi with the GMX_USE_RDTSCP=ON CMake option. Reading file benchMEM.tpr, VERSION 4.6.3-dev-20130701-6e3ae9e (single precision) Note: file tpx version 83, software tpx version 119 We can use A free GROMACS benchmark set to perform tests with this set .","title":"Example - running conteinerized gromacs in parallel"},{"location":"mpi_applications/#running-on-single-node","text":"","title":"Running on single node"},{"location":"mpi_applications/#image-based-mpi-runtime_1","text":"# Run shell in the container $ singularity shell Gromacs-openmpi20.sif # 2 MPI processes, 4 OpenMP threads per MPI process Singularity> mpirun -n 2 mdrun_mpi -ntomp 4 -s benchMEM.tpr -nsteps 10000 -resethway","title":"Image-based MPI runtime"},{"location":"mpi_applications/#host-based-mpi-runtime_1","text":"On my host which runs Ubuntu 20.04 the OpenMPI version is identical. On other machines you need to make sure to run compatible version. On HPC clusters this should be module which provides OpenMPI compiled with gcc. $ mpirun -n 2 singularity exec Gromacs-openmpi20.sif /usr/bin/mdrun_mpi -ntomp 4 -s benchMEM.tpr -nsteps 10000 -resethway # or relying on the runscript $ mpirun -n 2 Gromacs-openmpi20.sif -ntomp 4 -s benchMEM.tpr -nsteps 10000 -resethway Note: At present, there is a incompatibility in the OpenMPI version that causes \"Segmentation fault\" when trying to run it on Rackham@UPPMAX. Bootsraping from Ubntu 18.04 will install GROMACS - mdrun_mpi, 2018.1 and mpirun (Open MPI) 2.1.1 which \"resolves\" the problem and reveals other... Image-based MPI runtime: Run 2 MPI processes, 20 OpenMP threads per MPI process scontrol show hostname $SLURM_NODELIST > host singularity exec -B /etc/slurm:/etc/slurm-llnl -B /run/munge Gromacs-openmpi18.sif mpirun -n 2 -d -mca plm_base_verbose 10 --launch-agent 'singularity exec Gromacs-openmpi18.sif orted' /usr/bin/mdrun_mpi -ntomp 20 -s benchMEM.tpr -nsteps 10000 -resethway Use -d -mca plm_base_verbose 10 to get debugging information from mpirun . The slurm client that provides srun in Ubuntu 18.04 is too old to read the current slurm configuration on Rackham@UPPMAX. # Ubuntu 18.04 SLURM error # ============================ ... srun : error : _parse_next_key : Parsing error at unrecognized key : SlurmctldHost srun : error : Parse error in file / etc / slurm - llnl / slurm . conf line 5 : \"SlurmctldHost=rackham-q\" srun : error : _parse_next_key : Parsing error at unrecognized key : CredType srun : error : Parse error in file / etc / slurm - llnl / slurm . conf line 7 : \"CredType=cred/munge\" srun : fatal : Unable to process configuration file ... # Ubuntu 20.04 SLURM passes but gromacs+mpi binaries fail on Rackham # ===================================================================== ... [ r483 . uppmax . uu . se : 25362 ] [[ 34501 , 0 ], 0 ] plm : slurm : final top - level argv : srun -- ntasks - per - node = 1 -- kill - on - bad - exit -- nodes = 1 -- nodelist = r484 -- ntasks = 1 / usr / bin / singularity exec Gromacs - apt . sif / usr / bin / orted - mca ess \"slurm\" - mca ess _base_jobid \"2261057536\" - mca ess_base_vpid \"1\" - mca ess_base_num_procs \"2\" - mca orte_node_regex \"r[3:483-484]@0(2)\" - mca orte_hnp_uri \"2261057536.0;tcp://172.18.10.240,10.1.10.234,10.0.10.234:44203\" - mca plm_base_verbose \"10\" - mca - d \"- display - allocation \" [ r483 . uppmax . uu . se : 25362 ] [[ 34501 , 0 ], 0 ] complete_setup on job [ 34501 , 1 ] Data for JOB [ 34501 , 1 ] offset 0 Total slots allocated 2 2261057536.0 ; tcp : //172.18.10.240,10.1.10.234,10.0.10.234:44203 ... Reading file benchMEM . tpr , VERSION 4.6.3 - dev -20130701 -6e3 ae9e ( single precision ) Note : file tpx version 83 , software tpx version 119 [ r484 : 19455 ] *** Process received signal *** [ r484 : 19455 ] Signal : Segmentation fault ( 11 ) ... Attempting to start the jobs manually requires -mca plm rsh to skip SLURM and use rsh/ssh to start the processes on all allocated nodes and -B /etc/ssh to pick up the \"Host authentication\" setup. Here are the problems: Rackham@UPPMAX uses \" Host based authentication \" which does not work in the Singularity container, because the container runs in the user space and can not get/read the private host key... One can use passwordless user key for the authentication ( Note: passwordless ssh keys are not allowed on Rackham ) after adding all nodes public host keys/signatures ( the signatures that you are asked to accept when connecting for the first time to a machine ). Host-based MPI runtime: Run 2 MPI processes, 20 OpenMP threads per MPI process (explicitly specified on the mpirun command line) #!/bin/bash -l #SBATCH -J test #SBATCH -t 00:15:00 #SBATCH -p devel -N 2 -n 2 #SBATCH --cpus-per-task 20 #SBATCH -A project module load gcc/7.2.0 openmpi/2.1.1 mpirun -n 2 Gromacs-openmpi18.sif -ntomp 20 -s benchMEM.tpr -nsteps 10000 -resethway # or # mpirun -n 2 singularity exec Gromacs-openmpi18.sif /usr/bin/mdrun_mpi -ntomp 20 -s benchMEM.tpr -nsteps 10000 -resethway","title":"Host-based MPI runtime"},{"location":"mpi_applications/#compiling-gromacs","text":"Note: the recipe bellow uses the OpenMPI from the distribution that triggers \"Segmentation fault\" problems when running the container on Rackham@UPPMAX. recipe Bootstrap : docker From : ubuntu: 20.04 %setup %files %environment export LC_ALL = C %post export LC_ALL = C export DEBIAN_FRONTEND = noninteractive export NCPU = $( grep -c ^processor /proc/cpuinfo ) apt-get update && apt-get -y dist-upgrade && apt-get install -y git wget gawk cmake build-essential libopenmpi-dev openssh-client mkdir -p installs # Gromacs mkdir -p /tmp/downloads && cd /tmp/downloads test -f gromacs-2021.2.tar.gz || wget https://ftp.gromacs.org/gromacs/gromacs-2021.2.tar.gz tar xf gromacs-2021.2.tar.gz -C /installs cd /installs/gromacs-2021.2 mkdir build-normal && cd build-normal cmake .. -DCMAKE_INSTALL_PREFIX = /opt/gromacs-2021.2 -DGMX_GPU = OFF -DGMX_MPI = ON -DGMX_THREAD_MPI = ON -DGMX_BUILD_OWN_FFTW = ON -DGMX_DOUBLE = OFF -DGM X_PREFER_STATIC_LIBS = ON -DBUILD_SHARED_LIBS = ON -DCMAKE_BUILD_TYPE = RELEASE make -j $NCPU && make install cd /installs/gromacs-2021.2 mkdir build-mdrun-only && cd build-mdrun-only cmake .. -DCMAKE_INSTALL_PREFIX = /opt/gromacs-2021.2 -DGMX_GPU = OFF -DGMX_MPI = ON -DGMX_THREAD_MPI = ON -DGMX_BUILD_OWN_FFTW = ON -DGMX_DOUBLE = OFF -DGM X_PREFER_STATIC_LIBS = ON -DBUILD_SHARED_LIBS = ON -DCMAKE_BUILD_TYPE = RELEASE -DGMX_BUILD_MDRUN_ONLY = ON make -j $NCPU && make install cd / rm -r /installs rm /etc/apt/apt.conf.d/singularity-cache.conf %runscript #!/bin/bash source /opt/gromacs-2021.2/bin/GMXRC exec gmx_mpi \" $@ \" GROMACS reminds you: \"Statistics: The only science that enables different experts using the same figures to draw different conclusions.\" (Evan Esar)","title":"Compiling Gromacs"},{"location":"openmpi-benchmark/","text":"Singularity MPI ( Kebnekaise ) Setup The following definition script ( osu_benchmarks.def ) was used to generate the Singularity container: Bootstrap : docker From : ubuntu: 20.04 %files /home/pedro/Singularity_MPI/osu-micro-benchmarks-5.8.tgz /root/ /home/pedro/Singularity_MPI/openmpi-4.0.3.tar.gz /root/ %environment export OSU_DIR = /usr/local/osu/libexec/osu-micro-benchmarks/mpi %post apt-get -y update && DEBIAN_FRONTEND = noninteractive apt-get -y install build-essential gfortran openssh-server cd /root tar zxvf openmpi-4.0.3.tar.gz && cd openmpi-4.0.3 echo \"Configuring and building OpenMPI...\" ./configure --prefix = /usr && make all install clean export PATH = /usr/bin: $PATH export LD_LIBRARY_PATH = /usr/lib: $LD_LIBRARY_PATH cd /root tar zxvf osu-micro-benchmarks-5.8.tgz cd osu-micro-benchmarks-5.8/ echo \"Configuring and building OSU Micro-Benchmarks...\" ./configure --prefix = /usr/local/osu CC = /usr/bin/mpicc CXX = /usr/bin/mpicxx make -j2 && make install %runscript echo \"Rank - About to run: ${ OSU_DIR } / $* \" exec ${ OSU_DIR } / $* Generation of the container was done through: sudo singularity build ... ... The container is then transferred to Kebnekaise. Notice that the MPI versions that will be used in Kebnekaise need to match the ones used in the generation of the container. Running the container is done through the batch system, a typical run for the latency benchmark case is as follows: #!/bin/bash #SBATCH -A Project_ID #SBATCH -n 2 #SBATCH -t 00:10:00 #SBATCH --exclusive ml singularity/3.8.2 ml GCC/9.3.0 ml OpenMPI/4.0.3 #MPI compiled mpirun -np 2 /osu-micro-benchmarks/mpi/pt2pt/osu_latency #Singularity container mpirun -np 2 singularity run osu_benchmarks.sif pt2pt/osu_latency Benchmark results Fig.1 - Latency benchmark on Kebnekaise using the on-site MPI-compiled version (black), the singularity container (red) and singularity container compiled with infiniband libraries (orange). There is a 9.6% \"overhead\" in going from MPI-compiled (black) to singularity (red) for 4MB message. Fig.2 - All-to-all benchmark on Kebnekaise using the on-site MPI-compiled version (black), the singularity container (red) and singularity container compiled with infiniband libraries (orange). There is a 24.2% \"overhead\" in going from MPI-compiled (black) to singularity (red) for 1MB message.","title":"Singularity MPI@Kebnekaise"},{"location":"openmpi-benchmark/#singularity-mpi-kebnekaise","text":"","title":"Singularity MPI (Kebnekaise)"},{"location":"openmpi-benchmark/#setup","text":"The following definition script ( osu_benchmarks.def ) was used to generate the Singularity container: Bootstrap : docker From : ubuntu: 20.04 %files /home/pedro/Singularity_MPI/osu-micro-benchmarks-5.8.tgz /root/ /home/pedro/Singularity_MPI/openmpi-4.0.3.tar.gz /root/ %environment export OSU_DIR = /usr/local/osu/libexec/osu-micro-benchmarks/mpi %post apt-get -y update && DEBIAN_FRONTEND = noninteractive apt-get -y install build-essential gfortran openssh-server cd /root tar zxvf openmpi-4.0.3.tar.gz && cd openmpi-4.0.3 echo \"Configuring and building OpenMPI...\" ./configure --prefix = /usr && make all install clean export PATH = /usr/bin: $PATH export LD_LIBRARY_PATH = /usr/lib: $LD_LIBRARY_PATH cd /root tar zxvf osu-micro-benchmarks-5.8.tgz cd osu-micro-benchmarks-5.8/ echo \"Configuring and building OSU Micro-Benchmarks...\" ./configure --prefix = /usr/local/osu CC = /usr/bin/mpicc CXX = /usr/bin/mpicxx make -j2 && make install %runscript echo \"Rank - About to run: ${ OSU_DIR } / $* \" exec ${ OSU_DIR } / $* Generation of the container was done through: sudo singularity build ... ... The container is then transferred to Kebnekaise. Notice that the MPI versions that will be used in Kebnekaise need to match the ones used in the generation of the container. Running the container is done through the batch system, a typical run for the latency benchmark case is as follows: #!/bin/bash #SBATCH -A Project_ID #SBATCH -n 2 #SBATCH -t 00:10:00 #SBATCH --exclusive ml singularity/3.8.2 ml GCC/9.3.0 ml OpenMPI/4.0.3 #MPI compiled mpirun -np 2 /osu-micro-benchmarks/mpi/pt2pt/osu_latency #Singularity container mpirun -np 2 singularity run osu_benchmarks.sif pt2pt/osu_latency","title":"Setup"},{"location":"openmpi-benchmark/#benchmark-results","text":"Fig.1 - Latency benchmark on Kebnekaise using the on-site MPI-compiled version (black), the singularity container (red) and singularity container compiled with infiniband libraries (orange). There is a 9.6% \"overhead\" in going from MPI-compiled (black) to singularity (red) for 4MB message. Fig.2 - All-to-all benchmark on Kebnekaise using the on-site MPI-compiled version (black), the singularity container (red) and singularity container compiled with infiniband libraries (orange). There is a 24.2% \"overhead\" in going from MPI-compiled (black) to singularity (red) for 1MB message.","title":"Benchmark results"},{"location":"own_container/","text":"Building simple container Let's start with a simple example. For the more complicated we will try to use a bit more interactive approach using --sandbox option. Here we simply install the Paraview program in virtual environment conveniently provided by the Ubuntu distribution via the docker hub repository. Singularity.paraview Bootstrap : docker From : ubuntu: 20.04 %post export DEBIAN_FRONTEND = noninteractive apt-get update && apt-get -y dist-upgrade && \\ apt-get install -y paraview && \\ apt-get clean %runscript paraview \" $@ \" $ sudo singularity build paraview.sif Singularity.paraview This will download 301 MB and install 500 new packages... It might take some time to complete, but once you are done you will have a container that will run almost everywhere - there is always a catch. Instead of paraview, modify the definition file to install and run your, not necessarily graphical, program. Few tips: gnuplot , grace , blender , povray , rasmol , gromacs-openmpi ... Building interactively in a sandbox. Unless you know exactly which programs and commands you need to install it might be rather tricky to assemble a recipe that will work. If every change requires rebuilding it becomes rather tedious work. Instead, we can try to build a container as we would do it interactively on the command line. For this purpose, we will use --sandbox option which keeps the file structure intact. A regular build will create this folder structure in the /tmp then at the end will wrap everything in a single compressed read-only file by mksquasfs and delete the folder... The so-called sandbox is writable (by root) and accessible as regular folder (by root). This is extremely convenient for testing purposes. The sandbox folder could be later converted to a regular container - but please do not do it unless you have a good reason why you are breaking all the reproducible features of the container. Instead, during the interactive build, take notes by editing the definition file and build from scratch when you think you are ready. Let's try this with something which might or might not work - install jupyter with jupyter_contrib_nbextensions and jupyter_nbextensions_configurator via pip . Select location where you will create the new container-folder - in this case jupyter-sb $ sudo singularity build --sandbox jupyter-sb docker://ubuntu:20.04 INFO: Starting build... Getting image source signatures Copying blob 5d3b2c2d21bb skipped: already exists Copying blob 3fc2062ea667 skipped: already exists Copying blob 75adf526d75b [ -------------------------------------- ] 0 .0b / 0 .0b Copying config 591d30d91a done Writing manifest to image destination Storing signatures 2021 /03/15 13 :17:49 info unpack layer: sha256:5d3b2c2d21bba59850dac063bcbb574fddcb6aefb444ffcc63843355d878d54f 2021 /03/15 13 :17:51 info unpack layer: sha256:3fc2062ea6672189447be7510fb7d5bc2ef2fda234a04b457d9dda4bba5cc635 2021 /03/15 13 :17:51 info unpack layer: sha256:75adf526d75b82eb4f9981cce0b23608ebe6ab85c3e1ab2441f29b302d2f9aa8 INFO: Creating sandbox directory... INFO: Build complete: jupyter # List the current folder - note that the jupyter-sb is owned by root $ ls -l total 4 drwxr-xr-x 18 root root 4096 Mar 15 13 :17 jupyter-sb This pulls docker://ubuntu:20.04 image and build new singularity container in sandbox in folder jupyter-sb and we add two lines in the new recipe... Bootstrap : docker From : ubuntu: 20.04 Let's \"jump inside\" the container (ignore the warning) $ sudo singularity shell --writable jupyter-sb WARNING: Skipping mount /etc/localtime [ binds ] : /etc/localtime doesn ' t exist in container Singularity> Now, you are in the shell inside the container. Keep in mind that you are running as root at this point! $ cat /etc/os-release NAME = \"Ubuntu\" VERSION = \"20.04.2 LTS (Focal Fossa)\" ID = ubuntu ID_LIKE = debian PRETTY_NAME = \"Ubuntu 20.04.2 LTS\" VERSION_ID = \"20.04\" HOME_URL = \"https://www.ubuntu.com/\" SUPPORT_URL = \"https://help.ubuntu.com/\" BUG_REPORT_URL = \"https://bugs.launchpad.net/ubuntu/\" PRIVACY_POLICY_URL = \"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\" VERSION_CODENAME = focal UBUNTU_CODENAME = focal To be consistent, run export DEBIAN_FRONTEND=noninteractive ( otherwise you will be asked to provide localization interactively during the package installation ) and add this line to the %post section of the recipe. Singularity> export DEBIAN_FRONTEND = noninteractive Singularity> apt-get update Singularity> apt-get install -y locales python3-dev python3-pip python3-tk build-essential bash-completion Then the recipe becomes... Bootstrap : docker From : ubuntu: 20.04 %post export DEBIAN_FRONTEND = noninteractive apt-get update apt-get install -y locales python3-dev python3-pip python3-tk build-essential bash-completion We are ready for pip Singularity> python3 -m pip install --upgrade pip Singularity> python3 -m pip install jupyter Singularity> python3 -m pip install jupyter_contrib_nbextensions Singularity> jupyter contrib nbextension install --system Singularity> jupyter nbextension enable codefolding/main Singularity> python3 -m pip install jupyter_nbextensions_configurator Singularity> jupyter nbextensions_configurator enable --system Add the commands in the relevant section. Now, let's try to run the jupyter notebook. Singularity> jupyter notebook --ip 0 .0.0.0 --no-browser [ I 13 :46:05.654 NotebookApp ] Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret [ I 13 :46:05.946 NotebookApp ] [ jupyter_nbextensions_configurator ] enabled 0 .4.1 [ C 13 :46:05.946 NotebookApp ] Running as root is not recommended. Use --allow-root to bypass. It complains but it seems that it will work. Exit from the container exit . Add %runscrip . Try to build the recipe you have assembled by now. Singularity.jupyter Bootstrap : docker From : ubuntu: 20.04 %post export DEBIAN_FRONTEND = noninteractive apt-get update apt-get install -y locales python3-dev python3-pip python3-tk build-essential bash-completion rm -rf /var/lib/apt/lists/* python3 -m pip install --no-cache-dir --upgrade pip python3 -m pip install --no-cache-dir jupyter python3 -m pip install --no-cache-dir jupyter_contrib_nbextensions jupyter contrib nbextension install --system jupyter nbextension enable codefolding/main python3 -m pip install --no-cache-dir jupyter_nbextensions_configurator jupyter nbextensions_configurator enable --system %runscript jupyter notebook --ip 0 .0.0.0 --no-browser $ sudo singularity build jupyter.sif Singularity.jupyter Note, the added line rm -rf /var/lib/apt/lists/* and options --no-cache-dir to clean or skip remaining cached files. When it is done, try to run it with ./jupyter.sif . Does it work? Can you open the address with the browser? Yes, you can install whatever packages you want and they will be available and preset in the container. Keep in mind that you have installed all packages with pip in the container at system level and they will be available to any user running the container. Important ! If you or somebody else who will use the container have packages installed in their home folder i.e. pip install --user package they will come on top of everything - for god or bad... When you are done experimenting, and your recipe builds and works, do not forget to delete the sandbox. Be careful, you will need run rm -r with sudo . Slow down. Check twice when you run $ sudo rm -r jupyter-sb","title":"Build own container"},{"location":"own_container/#building-simple-container","text":"Let's start with a simple example. For the more complicated we will try to use a bit more interactive approach using --sandbox option. Here we simply install the Paraview program in virtual environment conveniently provided by the Ubuntu distribution via the docker hub repository. Singularity.paraview Bootstrap : docker From : ubuntu: 20.04 %post export DEBIAN_FRONTEND = noninteractive apt-get update && apt-get -y dist-upgrade && \\ apt-get install -y paraview && \\ apt-get clean %runscript paraview \" $@ \" $ sudo singularity build paraview.sif Singularity.paraview This will download 301 MB and install 500 new packages... It might take some time to complete, but once you are done you will have a container that will run almost everywhere - there is always a catch. Instead of paraview, modify the definition file to install and run your, not necessarily graphical, program. Few tips: gnuplot , grace , blender , povray , rasmol , gromacs-openmpi ...","title":"Building simple container"},{"location":"own_container/#building-interactively-in-a-sandbox","text":"Unless you know exactly which programs and commands you need to install it might be rather tricky to assemble a recipe that will work. If every change requires rebuilding it becomes rather tedious work. Instead, we can try to build a container as we would do it interactively on the command line. For this purpose, we will use --sandbox option which keeps the file structure intact. A regular build will create this folder structure in the /tmp then at the end will wrap everything in a single compressed read-only file by mksquasfs and delete the folder... The so-called sandbox is writable (by root) and accessible as regular folder (by root). This is extremely convenient for testing purposes. The sandbox folder could be later converted to a regular container - but please do not do it unless you have a good reason why you are breaking all the reproducible features of the container. Instead, during the interactive build, take notes by editing the definition file and build from scratch when you think you are ready. Let's try this with something which might or might not work - install jupyter with jupyter_contrib_nbextensions and jupyter_nbextensions_configurator via pip . Select location where you will create the new container-folder - in this case jupyter-sb $ sudo singularity build --sandbox jupyter-sb docker://ubuntu:20.04 INFO: Starting build... Getting image source signatures Copying blob 5d3b2c2d21bb skipped: already exists Copying blob 3fc2062ea667 skipped: already exists Copying blob 75adf526d75b [ -------------------------------------- ] 0 .0b / 0 .0b Copying config 591d30d91a done Writing manifest to image destination Storing signatures 2021 /03/15 13 :17:49 info unpack layer: sha256:5d3b2c2d21bba59850dac063bcbb574fddcb6aefb444ffcc63843355d878d54f 2021 /03/15 13 :17:51 info unpack layer: sha256:3fc2062ea6672189447be7510fb7d5bc2ef2fda234a04b457d9dda4bba5cc635 2021 /03/15 13 :17:51 info unpack layer: sha256:75adf526d75b82eb4f9981cce0b23608ebe6ab85c3e1ab2441f29b302d2f9aa8 INFO: Creating sandbox directory... INFO: Build complete: jupyter # List the current folder - note that the jupyter-sb is owned by root $ ls -l total 4 drwxr-xr-x 18 root root 4096 Mar 15 13 :17 jupyter-sb This pulls docker://ubuntu:20.04 image and build new singularity container in sandbox in folder jupyter-sb and we add two lines in the new recipe... Bootstrap : docker From : ubuntu: 20.04 Let's \"jump inside\" the container (ignore the warning) $ sudo singularity shell --writable jupyter-sb WARNING: Skipping mount /etc/localtime [ binds ] : /etc/localtime doesn ' t exist in container Singularity> Now, you are in the shell inside the container. Keep in mind that you are running as root at this point! $ cat /etc/os-release NAME = \"Ubuntu\" VERSION = \"20.04.2 LTS (Focal Fossa)\" ID = ubuntu ID_LIKE = debian PRETTY_NAME = \"Ubuntu 20.04.2 LTS\" VERSION_ID = \"20.04\" HOME_URL = \"https://www.ubuntu.com/\" SUPPORT_URL = \"https://help.ubuntu.com/\" BUG_REPORT_URL = \"https://bugs.launchpad.net/ubuntu/\" PRIVACY_POLICY_URL = \"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\" VERSION_CODENAME = focal UBUNTU_CODENAME = focal To be consistent, run export DEBIAN_FRONTEND=noninteractive ( otherwise you will be asked to provide localization interactively during the package installation ) and add this line to the %post section of the recipe. Singularity> export DEBIAN_FRONTEND = noninteractive Singularity> apt-get update Singularity> apt-get install -y locales python3-dev python3-pip python3-tk build-essential bash-completion Then the recipe becomes... Bootstrap : docker From : ubuntu: 20.04 %post export DEBIAN_FRONTEND = noninteractive apt-get update apt-get install -y locales python3-dev python3-pip python3-tk build-essential bash-completion We are ready for pip Singularity> python3 -m pip install --upgrade pip Singularity> python3 -m pip install jupyter Singularity> python3 -m pip install jupyter_contrib_nbextensions Singularity> jupyter contrib nbextension install --system Singularity> jupyter nbextension enable codefolding/main Singularity> python3 -m pip install jupyter_nbextensions_configurator Singularity> jupyter nbextensions_configurator enable --system Add the commands in the relevant section. Now, let's try to run the jupyter notebook. Singularity> jupyter notebook --ip 0 .0.0.0 --no-browser [ I 13 :46:05.654 NotebookApp ] Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret [ I 13 :46:05.946 NotebookApp ] [ jupyter_nbextensions_configurator ] enabled 0 .4.1 [ C 13 :46:05.946 NotebookApp ] Running as root is not recommended. Use --allow-root to bypass. It complains but it seems that it will work. Exit from the container exit . Add %runscrip . Try to build the recipe you have assembled by now. Singularity.jupyter Bootstrap : docker From : ubuntu: 20.04 %post export DEBIAN_FRONTEND = noninteractive apt-get update apt-get install -y locales python3-dev python3-pip python3-tk build-essential bash-completion rm -rf /var/lib/apt/lists/* python3 -m pip install --no-cache-dir --upgrade pip python3 -m pip install --no-cache-dir jupyter python3 -m pip install --no-cache-dir jupyter_contrib_nbextensions jupyter contrib nbextension install --system jupyter nbextension enable codefolding/main python3 -m pip install --no-cache-dir jupyter_nbextensions_configurator jupyter nbextensions_configurator enable --system %runscript jupyter notebook --ip 0 .0.0.0 --no-browser $ sudo singularity build jupyter.sif Singularity.jupyter Note, the added line rm -rf /var/lib/apt/lists/* and options --no-cache-dir to clean or skip remaining cached files. When it is done, try to run it with ./jupyter.sif . Does it work? Can you open the address with the browser? Yes, you can install whatever packages you want and they will be available and preset in the container. Keep in mind that you have installed all packages with pip in the container at system level and they will be available to any user running the container. Important ! If you or somebody else who will use the container have packages installed in their home folder i.e. pip install --user package they will come on top of everything - for god or bad... When you are done experimenting, and your recipe builds and works, do not forget to delete the sandbox. Be careful, you will need run rm -r with sudo . Slow down. Check twice when you run $ sudo rm -r jupyter-sb","title":"Building interactively in a sandbox."},{"location":"practical_information/","text":"Practical information Information updated: 2022.03.31 - Place: The workshop will be online via Zoom. Link will be sent a week before the workshop . - Time: - 20 April 2022 - 09:15-12:00 - 13:15-16:00 ( If you need help with the workshop setup, the meeting will be open from 9:00 ) - Workshop material online: https://pmitev.github.io/UPPMAX-Singularity-workshop/ - ShellShare - link will be shared diring the workshop. Live - HackMD page to communicate latest information diring the workshop.","title":"Practical information"},{"location":"practical_information/#practical-information","text":"Information updated: 2022.03.31 - Place: The workshop will be online via Zoom. Link will be sent a week before the workshop . - Time: - 20 April 2022 - 09:15-12:00 - 13:15-16:00 ( If you need help with the workshop setup, the meeting will be open from 9:00 ) - Workshop material online: https://pmitev.github.io/UPPMAX-Singularity-workshop/ - ShellShare - link will be shared diring the workshop. Live - HackMD page to communicate latest information diring the workshop.","title":"Practical information"},{"location":"remote_builder/","text":"Building containers remotely Building Singularity containers require sudo with root access which usually is not the case on community or public computer resources. Instances running on the cloud are an exception of this general rule. In this course we will limit to exercises with local builds and try building remote when we gather some experience. For now, here are the two most common remote building services (GitHub is experimenting with it as well). Singularity Container Services Default installation of Singularity is configured to connect to the public cloud.sylabs.io services which allows you to send a definition file to be built on the Sylab cloud. Follow the manual about Remote Endpoinst to learn how to build containers remotely. Singularity Container Registry (Singularity Hub) Singularity Hub is the predecessor to Singularity Registry, and while it also serves as an image registry, in addition it provides a cloud build service for users. Singularity Hub also takes advantage of Github for version control of build recipes. The user pushes to Github, a builder is deployed, and the image available to the user. Singularity Hub would allow a user to build and run an image from a resource where he or she doesn't have sudo simply by using Github as a middleman. Notice Singularity Hub is no longer online as a builder service, but exists as a read only archive. Containers built before April 19, 2021 are available at their same pull URLs. To see a last day gallery of Singularity Hub, please see here","title":"Building remotely"},{"location":"remote_builder/#building-containers-remotely","text":"Building Singularity containers require sudo with root access which usually is not the case on community or public computer resources. Instances running on the cloud are an exception of this general rule. In this course we will limit to exercises with local builds and try building remote when we gather some experience. For now, here are the two most common remote building services (GitHub is experimenting with it as well).","title":"Building containers remotely"},{"location":"remote_builder/#singularity-container-services","text":"Default installation of Singularity is configured to connect to the public cloud.sylabs.io services which allows you to send a definition file to be built on the Sylab cloud. Follow the manual about Remote Endpoinst to learn how to build containers remotely.","title":"Singularity Container Services"},{"location":"remote_builder/#singularity-container-registry-singularity-hub","text":"Singularity Hub is the predecessor to Singularity Registry, and while it also serves as an image registry, in addition it provides a cloud build service for users. Singularity Hub also takes advantage of Github for version control of build recipes. The user pushes to Github, a builder is deployed, and the image available to the user. Singularity Hub would allow a user to build and run an image from a resource where he or she doesn't have sudo simply by using Github as a middleman. Notice Singularity Hub is no longer online as a builder service, but exists as a read only archive. Containers built before April 19, 2021 are available at their same pull URLs. To see a last day gallery of Singularity Hub, please see here","title":"Singularity Container Registry (Singularity Hub)"},{"location":"remote_repositories/","text":"Running Singlarity containers from online repositories Here we will briefly show one example and later try to build our own container and share it online. $ singularity run docker://godlovedc/lolcow INFO: Converting OCI blobs to SIF format INFO: Starting build... Getting image source signatures Copying blob 9fb6c798fa41 done Copying blob 3b61febd4aef done Copying blob 9d99b9777eb0 done Copying blob d010c8cf75d7 done Copying blob 7fac07fb303e done Copying blob 8e860504ff1e done Copying config 73d5b1025f done Writing manifest to image destination Storing signatures ... 2021 /03/15 11 :18:19 info unpack layer: sha256:3b61febd4aefe982e0cb9c696d415137384d1a01052b50a85aae46439e15e49a 2021 /03/15 11 :18:19 info unpack layer: sha256:9d99b9777eb02b8943c0e72d7a7baec5c782f8fd976825c9d3fb48b3101aacc2 2021 /03/15 11 :18:19 info unpack layer: sha256:d010c8cf75d7eb5d2504d5ffa0d19696e8d745a457dd8d28ec6dd41d3763617e 2021 /03/15 11 :18:19 info unpack layer: sha256:7fac07fb303e0589b9c23e6f49d5dc1ff9d6f3c8c88cabe768b430bdb47f03a9 2021 /03/15 11 :18:19 info unpack layer: sha256:8e860504ff1ee5dc7953672d128ce1e4aa4d8e3716eb39fe710b849c64b20945 INFO: Creating SIF file... __________________________________ / Someone is speaking well of you. \\ | | \\ How unusual! / ---------------------------------- \\ ^__^ \\ ( oo ) \\_ ______ ( __ ) \\ ) \\/\\ || ----w | || || Let's run it again. $ singularity run docker://godlovedc/lolcow INFO: Using cached SIF image ___________________________ < You are as I am with You. > --------------------------- \\ ^__^ \\ ( oo ) \\_ ______ ( __ ) \\ ) \\/\\ || ----w | || || Note, that singularity, after contacting the repositories, realizes that the container is in the local cache and proceeds to run it. But where is it? More details... $ singularity cache list There are 1 container file ( s ) using 87 .96 MiB and 8 oci blob file ( s ) using 99 .09 MiB of space Total space used: 187 .04 MiB Info Over time the cache will grow and might easily accumulate unnecessary \"blobs\". To clean the cache you can run. $ singularity cache clean Here is how the cache might look like: singularity cache clean --dry-run User requested a dry run. Not actually deleting any data! INFO: Removing blob cache entry: blobs INFO: Removing blob cache entry: index.json INFO: Removing blob cache entry: oci-layout INFO: No cached files to remove at /home/ubuntu/.singularity/ cache/library INFO: Removing oci-tmp cache entry: a692b57abc43035b197b10390ea2c12855d21649f2ea2cc28094d18b93360eeb INFO: No cached files to remove at /home/ubuntu/.singularity/ cache/shub INFO: No cached files to remove at /home/ubuntu/.singularity/ cache/oras INFO: No cached files to remove at /home/ubuntu/.singularity/ cache/net More examples $ singularity run docker://dctrud/wttr Tensorflow Let's have some tensorflow running. First pull the image from docker hub (~2.6GB). $ singularity pull docker://tensorflow/tensorflow:latest-gpu INFO: Converting OCI blobs to SIF format INFO: Starting build... Getting image source signatures ... If you have a GPU card, here is how easy you can get tensorflow running. Note the --nv option on the command line. $ singularity exec -- nv tensorflow_latest - gpu . sif python3 Python 3.8.10 ( default , Nov 26 2021 , 20 : 14 : 08 ) [ GCC 9.3.0 ] on linux Type \"help\" , \"copyright\" , \"credits\" or \"license\" for more information . >>> import tensorflow as tf >>> tf . config . list_physical_devices ( 'GPU' ) [ PhysicalDevice ( name = '/physical_device:GPU:0' , device_type = 'GPU' )] metaWRAP - a flexible pipeline for genome-resolved metagenomic data analysis Here is an example how to use the metaWRAP pipeline from the docker container - installation instructions . # Original instructions (do NOT run) $ docker pull quay.io/biocontainers/metawrap:1.2--1 In this particular case it is as easy as: $ singularity pull docker://quay.io/biocontainers/metawrap:1.2--1 INFO: Converting OCI blobs to SIF format INFO: Starting build... Getting image source signatures ... This will bring the docker container locally and covert it to Singularity format. Then, one can start the container and use it interactively. $ ./metawrap_1.2--1.sif WARNING: Skipping mount /usr/local/var/singularity/mnt/session/etc/resolv.conf [files]: /etc/resolv.conf doesn't exist in container Singularity> metawrap --version metaWRAP v=1.2 To run the tool from the command line (as you would use it in scripts) we need to add the call for the tool from Singularity. Original commad in the cript: $ metawrap binning -o Lanna-straw_initial_binning_concoct -t 20 -a /proj/test/megahit_ass_Lanna-straw/final.contigs.fa --concoct --run-checkm /proj/test/Lanna-straw_reads_trimmed/*.fastq The command now calls the tool from the Singularity container: $ singularity exec metawrap_1.2--1.sif metawrap binning -o Lanna-straw_initial_binning_concoct -t 20 -a /proj/test/megahit_ass_Lanna-straw/final.contigs.fa --concoct --run-checkm /proj/test/Lanna-straw_reads_trimmed/*.fastq Pulling Singularity container from online or local library/repository library:// to build from the Container Library library://sylabs-jms/testing/lolcow docker:// to build from Docker Hub docker://godlovedc/lolcow shub:// to build from Singularity Hub path to a existing container on your local machine path to a directory to build from a sandbox path to a Singularity definition file","title":"Running containrs from online repositories"},{"location":"remote_repositories/#running-singlarity-containers-from-online-repositories","text":"Here we will briefly show one example and later try to build our own container and share it online. $ singularity run docker://godlovedc/lolcow INFO: Converting OCI blobs to SIF format INFO: Starting build... Getting image source signatures Copying blob 9fb6c798fa41 done Copying blob 3b61febd4aef done Copying blob 9d99b9777eb0 done Copying blob d010c8cf75d7 done Copying blob 7fac07fb303e done Copying blob 8e860504ff1e done Copying config 73d5b1025f done Writing manifest to image destination Storing signatures ... 2021 /03/15 11 :18:19 info unpack layer: sha256:3b61febd4aefe982e0cb9c696d415137384d1a01052b50a85aae46439e15e49a 2021 /03/15 11 :18:19 info unpack layer: sha256:9d99b9777eb02b8943c0e72d7a7baec5c782f8fd976825c9d3fb48b3101aacc2 2021 /03/15 11 :18:19 info unpack layer: sha256:d010c8cf75d7eb5d2504d5ffa0d19696e8d745a457dd8d28ec6dd41d3763617e 2021 /03/15 11 :18:19 info unpack layer: sha256:7fac07fb303e0589b9c23e6f49d5dc1ff9d6f3c8c88cabe768b430bdb47f03a9 2021 /03/15 11 :18:19 info unpack layer: sha256:8e860504ff1ee5dc7953672d128ce1e4aa4d8e3716eb39fe710b849c64b20945 INFO: Creating SIF file... __________________________________ / Someone is speaking well of you. \\ | | \\ How unusual! / ---------------------------------- \\ ^__^ \\ ( oo ) \\_ ______ ( __ ) \\ ) \\/\\ || ----w | || || Let's run it again. $ singularity run docker://godlovedc/lolcow INFO: Using cached SIF image ___________________________ < You are as I am with You. > --------------------------- \\ ^__^ \\ ( oo ) \\_ ______ ( __ ) \\ ) \\/\\ || ----w | || || Note, that singularity, after contacting the repositories, realizes that the container is in the local cache and proceeds to run it. But where is it? More details... $ singularity cache list There are 1 container file ( s ) using 87 .96 MiB and 8 oci blob file ( s ) using 99 .09 MiB of space Total space used: 187 .04 MiB Info Over time the cache will grow and might easily accumulate unnecessary \"blobs\". To clean the cache you can run. $ singularity cache clean Here is how the cache might look like: singularity cache clean --dry-run User requested a dry run. Not actually deleting any data! INFO: Removing blob cache entry: blobs INFO: Removing blob cache entry: index.json INFO: Removing blob cache entry: oci-layout INFO: No cached files to remove at /home/ubuntu/.singularity/ cache/library INFO: Removing oci-tmp cache entry: a692b57abc43035b197b10390ea2c12855d21649f2ea2cc28094d18b93360eeb INFO: No cached files to remove at /home/ubuntu/.singularity/ cache/shub INFO: No cached files to remove at /home/ubuntu/.singularity/ cache/oras INFO: No cached files to remove at /home/ubuntu/.singularity/ cache/net","title":"Running Singlarity containers from online repositories"},{"location":"remote_repositories/#more-examples","text":"$ singularity run docker://dctrud/wttr","title":"More examples"},{"location":"remote_repositories/#tensorflow","text":"Let's have some tensorflow running. First pull the image from docker hub (~2.6GB). $ singularity pull docker://tensorflow/tensorflow:latest-gpu INFO: Converting OCI blobs to SIF format INFO: Starting build... Getting image source signatures ... If you have a GPU card, here is how easy you can get tensorflow running. Note the --nv option on the command line. $ singularity exec -- nv tensorflow_latest - gpu . sif python3 Python 3.8.10 ( default , Nov 26 2021 , 20 : 14 : 08 ) [ GCC 9.3.0 ] on linux Type \"help\" , \"copyright\" , \"credits\" or \"license\" for more information . >>> import tensorflow as tf >>> tf . config . list_physical_devices ( 'GPU' ) [ PhysicalDevice ( name = '/physical_device:GPU:0' , device_type = 'GPU' )]","title":"Tensorflow"},{"location":"remote_repositories/#metawrap-a-flexible-pipeline-for-genome-resolved-metagenomic-data-analysis","text":"Here is an example how to use the metaWRAP pipeline from the docker container - installation instructions . # Original instructions (do NOT run) $ docker pull quay.io/biocontainers/metawrap:1.2--1 In this particular case it is as easy as: $ singularity pull docker://quay.io/biocontainers/metawrap:1.2--1 INFO: Converting OCI blobs to SIF format INFO: Starting build... Getting image source signatures ... This will bring the docker container locally and covert it to Singularity format. Then, one can start the container and use it interactively. $ ./metawrap_1.2--1.sif WARNING: Skipping mount /usr/local/var/singularity/mnt/session/etc/resolv.conf [files]: /etc/resolv.conf doesn't exist in container Singularity> metawrap --version metaWRAP v=1.2 To run the tool from the command line (as you would use it in scripts) we need to add the call for the tool from Singularity. Original commad in the cript: $ metawrap binning -o Lanna-straw_initial_binning_concoct -t 20 -a /proj/test/megahit_ass_Lanna-straw/final.contigs.fa --concoct --run-checkm /proj/test/Lanna-straw_reads_trimmed/*.fastq The command now calls the tool from the Singularity container: $ singularity exec metawrap_1.2--1.sif metawrap binning -o Lanna-straw_initial_binning_concoct -t 20 -a /proj/test/megahit_ass_Lanna-straw/final.contigs.fa --concoct --run-checkm /proj/test/Lanna-straw_reads_trimmed/*.fastq Pulling Singularity container from online or local library/repository library:// to build from the Container Library library://sylabs-jms/testing/lolcow docker:// to build from Docker Hub docker://godlovedc/lolcow shub:// to build from Singularity Hub path to a existing container on your local machine path to a directory to build from a sandbox path to a Singularity definition file","title":"metaWRAP - a flexible pipeline for genome-resolved metagenomic data analysis"},{"location":"running_singularity/","text":"Running Singularitry container Let's practice a bit running one of the containers from the previous step. Run the Singularity container Executing the container file itself or a remote build will start will execute the commands defined in the %runscript section. This is equivalent to run singularity run ./lolcow.sif $ ./lolcow.sif _________________________________________ / You will stop at nothing to reach your \\ | objective, but only because your brakes | \\ are defective. / ----------------------------------------- \\ ^__^ \\ (oo)\\_______ (__)\\ )\\/\\ ||----w | || || $ singularity run ./lolcow.sif _____________________________________ / Don't relax! It's only your tension \\ \\ that's holding you together. / ------------------------------------- \\ ^__^ \\ (oo)\\_______ (__)\\ )\\/\\ ||----w | || || Getting a shell in the running container $ singularity shell ./lolcow.sif Singularity> If you try to list the files in your home folder with ls -l ~ you will see the content of your home folder. What about the root folder? List the content with ls -l / and compare the output from a different terminal. Singularity binds the user home folder, /tmp and some other by default. If you are running on a computer with more users accounts than your own (like on a computer cluster) compare the content of ls -l /home from within the container and outside. You should not be able to see the other users' folders that are on the computer otherwise. Binding/mounting folders By default, Singularity binds these folders from the host computer (your computer) to the container defined in /usr/local/etc/singularity/singularity.conf . 1 2 3 4 5 6 7 8 9 mount home = yes mount tmp = yes #bind path = /etc/singularity/default-nsswitch.conf:/etc/nsswitch.conf #bind path = /opt #bind path = /scratch bind path = /etc/localtime bind path = /etc/hosts On Rackham few more folders are automatically mounted for almost obvious reasons. - /scratch - /sw - /proj Binding folders from the host to the container, allows you to \"Inject\" any host folder as read/write folder in the container frequently used for external databases or standalone tools Replace container's folder or file form the host usually used to provide external configuration files to replace default common. More detailed online at User-defined bind paths Execute program in the container There is of course a way to start a different program than the default shell or the defined in the %runscript section. $ singularity exec ./lolcow.sif fortune Beauty and harmony are as necessary to you as the very breath of life. $ singularity exec ./lolcow.sif host FATAL: \"host\" : executable file not found in $PATH Keep in mind that running in the container you should be able to find the program inside or in the folders you have binded to the container i.e. the system tools and programs remain isolated. Warning If you have setup conda or pip installations in you profile folder, they get available in the container. This might conflict with the container setup which is unaware of programs installed in your home folder. To avoid such situations, you might need to run singularity with --cleanenv option i.e. singularity run -e ./lolcow.sif","title":"Running Singularity container"},{"location":"running_singularity/#running-singularitry-container","text":"Let's practice a bit running one of the containers from the previous step.","title":"Running Singularitry container"},{"location":"running_singularity/#run-the-singularity-container","text":"Executing the container file itself or a remote build will start will execute the commands defined in the %runscript section. This is equivalent to run singularity run ./lolcow.sif $ ./lolcow.sif _________________________________________ / You will stop at nothing to reach your \\ | objective, but only because your brakes | \\ are defective. / ----------------------------------------- \\ ^__^ \\ (oo)\\_______ (__)\\ )\\/\\ ||----w | || || $ singularity run ./lolcow.sif _____________________________________ / Don't relax! It's only your tension \\ \\ that's holding you together. / ------------------------------------- \\ ^__^ \\ (oo)\\_______ (__)\\ )\\/\\ ||----w | || ||","title":"Run the Singularity container"},{"location":"running_singularity/#getting-a-shell-in-the-running-container","text":"$ singularity shell ./lolcow.sif Singularity> If you try to list the files in your home folder with ls -l ~ you will see the content of your home folder. What about the root folder? List the content with ls -l / and compare the output from a different terminal. Singularity binds the user home folder, /tmp and some other by default. If you are running on a computer with more users accounts than your own (like on a computer cluster) compare the content of ls -l /home from within the container and outside. You should not be able to see the other users' folders that are on the computer otherwise.","title":"Getting a shell in the running container"},{"location":"running_singularity/#bindingmounting-folders","text":"By default, Singularity binds these folders from the host computer (your computer) to the container defined in /usr/local/etc/singularity/singularity.conf . 1 2 3 4 5 6 7 8 9 mount home = yes mount tmp = yes #bind path = /etc/singularity/default-nsswitch.conf:/etc/nsswitch.conf #bind path = /opt #bind path = /scratch bind path = /etc/localtime bind path = /etc/hosts On Rackham few more folders are automatically mounted for almost obvious reasons. - /scratch - /sw - /proj Binding folders from the host to the container, allows you to \"Inject\" any host folder as read/write folder in the container frequently used for external databases or standalone tools Replace container's folder or file form the host usually used to provide external configuration files to replace default common. More detailed online at User-defined bind paths","title":"Binding/mounting folders"},{"location":"running_singularity/#execute-program-in-the-container","text":"There is of course a way to start a different program than the default shell or the defined in the %runscript section. $ singularity exec ./lolcow.sif fortune Beauty and harmony are as necessary to you as the very breath of life. $ singularity exec ./lolcow.sif host FATAL: \"host\" : executable file not found in $PATH Keep in mind that running in the container you should be able to find the program inside or in the folders you have binded to the container i.e. the system tools and programs remain isolated. Warning If you have setup conda or pip installations in you profile folder, they get available in the container. This might conflict with the container setup which is unaware of programs installed in your home folder. To avoid such situations, you might need to run singularity with --cleanenv option i.e. singularity run -e ./lolcow.sif","title":"Execute program in the container"},{"location":"uppmax_in_a_can/","text":"UPPMAX in a can https://github.com/UPPMAX/uppmax_in_a_can This Singularity container will let you run a near-identical UPPMAX environment on your own computer. You will have access to all of the installed software at UPPMAX, all your files and reference data on UPPMAX, but it will be your own computer that does the calculations. You can even use it to analyse data that you only have on your own computer, but using the software and reference data on UPPMAX. Typical use cases You have sensitive data that is not allowed to leave your computers, but you want to use the programs and references at UPPMAX to do the analysis. You have your own server but want to avoid installing all the software yourself. The queues at UPPMAX are too long or you have run out of core hours but you want the analysis done yesterday. You have your data and compute hours at another SNIC center, but want to use the software installed at UPPMAX. What you get Access to your home folder and all project folders. Access to all the installed programs at UPPMAX. Access to all reference data at UPPMAX. What you don't get UPPMAX high-performance computers. You will be limited by the computer you are running the container on. No slurm access. Everything runs on your computer.","title":"UPPMAX in a can"},{"location":"uppmax_in_a_can/#uppmax-in-a-can","text":"https://github.com/UPPMAX/uppmax_in_a_can This Singularity container will let you run a near-identical UPPMAX environment on your own computer. You will have access to all of the installed software at UPPMAX, all your files and reference data on UPPMAX, but it will be your own computer that does the calculations. You can even use it to analyse data that you only have on your own computer, but using the software and reference data on UPPMAX.","title":"UPPMAX in a can"},{"location":"uppmax_in_a_can/#typical-use-cases","text":"You have sensitive data that is not allowed to leave your computers, but you want to use the programs and references at UPPMAX to do the analysis. You have your own server but want to avoid installing all the software yourself. The queues at UPPMAX are too long or you have run out of core hours but you want the analysis done yesterday. You have your data and compute hours at another SNIC center, but want to use the software installed at UPPMAX.","title":"Typical use cases"},{"location":"uppmax_in_a_can/#what-you-get","text":"Access to your home folder and all project folders. Access to all the installed programs at UPPMAX. Access to all reference data at UPPMAX.","title":"What you get"},{"location":"uppmax_in_a_can/#what-you-dont-get","text":"UPPMAX high-performance computers. You will be limited by the computer you are running the container on. No slurm access. Everything runs on your computer.","title":"What you don't get"}]}